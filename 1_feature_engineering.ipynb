{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f603800-bf27-4dd9-a9a7-caa5b9f9aa70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b808b954-953a-4069-ae88-29b633a726c4",
   "metadata": {},
   "source": [
    "**Combining Datasets Together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89ae30de-b0f7-49ac-8468-629f47f37484",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 2016\n",
    "end_year = 2023\n",
    "folder_names = np.array([[f'{k}Q{i}_cf' for i in range(1, 5)] for k in range(start_year, end_year+1)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2061e813-a52e-4371-bc79-a5c2dbca55dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2016Q2_cf', '2016Q3_cf', '2016Q4_cf', '2017Q1_cf', '2017Q2_cf',\n",
       "       '2017Q3_cf', '2017Q4_cf', '2018Q1_cf', '2018Q2_cf', '2018Q3_cf',\n",
       "       '2018Q4_cf', '2019Q1_cf', '2019Q2_cf', '2019Q3_cf', '2019Q4_cf',\n",
       "       '2020Q1_cf', '2020Q2_cf', '2020Q3_cf', '2020Q4_cf', '2021Q1_cf',\n",
       "       '2021Q2_cf', '2021Q3_cf', '2021Q4_cf', '2022Q1_cf', '2022Q2_cf',\n",
       "       '2022Q3_cf', '2022Q4_cf', '2023Q1_cf', '2023Q2_cf', '2023Q3_cf',\n",
       "       '2023Q4_cf'], dtype='<U9')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_names[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d46307df-77fc-4675-8fb8-9a5e5551bdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant file names\n",
    "file_name_submission = 'FORM_C_SUBMISSION.csv'\n",
    "file_name_issuer_info = 'FORM_C_ISSUER_INFORMATION.csv'\n",
    "file_name_disclosure = 'FORM_C_DISCLOSURE.csv'\n",
    "file_name_issuer_signature = 'FORM_C_ISSUER_SIGNATURE.csv'\n",
    "file_names = [file_name_issuer_info, file_name_disclosure, file_name_issuer_signature] # exclude SUBMISSION.csv\n",
    "\n",
    "start_year = 2016\n",
    "end_year = 2023\n",
    "folder_names = np.array([[f'{k}Q{i}_cf' for i in range(1, 5)] for k in range(start_year, end_year+1)]).flatten()\n",
    "folder_names = folder_names[1:] # Drop 2016Q1_cf because it does not exist\n",
    "\n",
    "file_path = 'sec_dataset/{}/{}'\n",
    "\n",
    "# Vertically union between files\n",
    "df = pd.DataFrame()\n",
    "for folder_name in folder_names:\n",
    "    # Horizontally join between files, of the same quarter\n",
    "    df_hor = pd.read_csv(file_path.format(folder_name, file_name_submission))\n",
    "    for file_name in file_names:\n",
    "        df_temp = pd.read_csv(file_path.format(folder_name, file_name))\n",
    "        # perform left-join\n",
    "        df_hor = pd.merge(left=df_hor, right=df_temp, on='ACCESSION_NUMBER', how='left')\n",
    "    \n",
    "    # perform union\n",
    "    df = pd.concat([df, df_hor], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239232b4-f14e-4646-8387-95846169f43d",
   "metadata": {},
   "source": [
    "**Ensure each column has the correct data type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba85bed6-df55-4f43-aa60-82edc98f656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68a26c0b-a490-426f-9b1c-b6b4e6dac59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FILING_DATE -> From int64 to Date\n",
    "df_cleaned['FILING_DATE'] = pd.to_datetime(df['FILING_DATE'].astype(str), format='%Y%m%d')\n",
    "\n",
    "# DATEINCORPORATION -> From object to Date\n",
    "df_cleaned['DATEINCORPORATION'] = pd.to_datetime(df['DATEINCORPORATION'].astype(str))\n",
    "\n",
    "# OVERSUBSCRIPTIONACCEPTED -> From object to integer (where Y and N become 1 and 0, respectively)\n",
    "df_cleaned['OVERSUBSCRIPTIONACCEPTED'] = df['OVERSUBSCRIPTIONACCEPTED'].map({'Y':1, 'N':0})\n",
    "\n",
    "# DEADLINEDATE -> From object to Date\n",
    "df_cleaned['DEADLINEDATE'] = pd.to_datetime(df['DEADLINEDATE'].astype(str), format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cd9f452-41ae-4149-bbf8-05db575bd73a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25436 entries, 0 to 25435\n",
      "Data columns (total 61 columns):\n",
      " #   Column                          Non-Null Count  Dtype         \n",
      "---  ------                          --------------  -----         \n",
      " 0   ACCESSION_NUMBER                25436 non-null  object        \n",
      " 1   SUBMISSION_TYPE                 25436 non-null  object        \n",
      " 2   FILING_DATE                     25436 non-null  datetime64[ns]\n",
      " 3   CIK                             25434 non-null  float64       \n",
      " 4   FILE_NUMBER                     25436 non-null  object        \n",
      " 5   PERIOD                          3400 non-null   float64       \n",
      " 6   ISAMENDMENT                     25436 non-null  int64         \n",
      " 7   PROGRESSUPDATE                  3948 non-null   object        \n",
      " 8   NATUREOFAMENDMENT               8543 non-null   object        \n",
      " 9   NAMEOFISSUER                    25434 non-null  object        \n",
      " 10  LEGALSTATUSFORM                 24501 non-null  object        \n",
      " 11  LEGALSTATUSOTHERDESC            301 non-null    object        \n",
      " 12  JURISDICTIONORGANIZATION        24501 non-null  object        \n",
      " 13  DATEINCORPORATION               24501 non-null  datetime64[ns]\n",
      " 14  STREET1                         24500 non-null  object        \n",
      " 15  STREET2                         7729 non-null   object        \n",
      " 16  CITY                            24501 non-null  object        \n",
      " 17  STATEORCOUNTRY                  24501 non-null  object        \n",
      " 18  ZIPCODE                         24501 non-null  object        \n",
      " 19  ISSUERWEBSITE                   24494 non-null  object        \n",
      " 20  COMPANYNAME                     20532 non-null  object        \n",
      " 21  COMMISSIONCIK                   20532 non-null  float64       \n",
      " 22  COMMISSIONFILENUMBER            20532 non-null  object        \n",
      " 23  CRDNUMBER                       12770 non-null  float64       \n",
      " 24  COMPENSATIONAMOUNT              20445 non-null  object        \n",
      " 25  FINANCIALINTEREST               17518 non-null  object        \n",
      " 26  SECURITYOFFEREDTYPE             20446 non-null  object        \n",
      " 27  SECURITYOFFEREDOTHERDESC        8807 non-null   object        \n",
      " 28  NOOFSECURITYOFFERED             16245 non-null  float64       \n",
      " 29  PRICE                           19443 non-null  float64       \n",
      " 30  PRICEDETERMINATIONMETHOD        11917 non-null  object        \n",
      " 31  OFFERINGAMOUNT                  20446 non-null  float64       \n",
      " 32  OVERSUBSCRIPTIONACCEPTED        20446 non-null  float64       \n",
      " 33  OVERSUBSCRIPTIONALLOCATIONTYPE  20181 non-null  object        \n",
      " 34  DESCOVERSUBSCRIPTION            11739 non-null  object        \n",
      " 35  MAXIMUMOFFERINGAMOUNT           20181 non-null  float64       \n",
      " 36  DEADLINEDATE                    20446 non-null  datetime64[ns]\n",
      " 37  CURRENTEMPLOYEES                23837 non-null  float64       \n",
      " 38  TOTALASSETMOSTRECENTFISCALYEAR  23837 non-null  float64       \n",
      " 39  TOTALASSETPRIORFISCALYEAR       23837 non-null  float64       \n",
      " 40  CASHEQUIMOSTRECENTFISCALYEAR    23837 non-null  float64       \n",
      " 41  CASHEQUIPRIORFISCALYEAR         23837 non-null  float64       \n",
      " 42  ACTRECEIVEDRECENTFISCALYEAR     23837 non-null  float64       \n",
      " 43  ACTRECEIVEDPRIORFISCALYEAR      23837 non-null  float64       \n",
      " 44  SHORTTERMDEBTMRECENTFISCALYEAR  23837 non-null  float64       \n",
      " 45  SHORTTERMDEBTPRIORFISCALYEAR    23837 non-null  float64       \n",
      " 46  LONGTERMDEBTRECENTFISCALYEAR    23837 non-null  float64       \n",
      " 47  LONGTERMDEBTPRIORFISCALYEAR     23837 non-null  float64       \n",
      " 48  REVENUEMOSTRECENTFISCALYEAR     23837 non-null  float64       \n",
      " 49  REVENUEPRIORFISCALYEAR          23837 non-null  float64       \n",
      " 50  COSTGOODSSOLDRECENTFISCALYEAR   23837 non-null  float64       \n",
      " 51  COSTGOODSSOLDPRIORFISCALYEAR    23837 non-null  float64       \n",
      " 52  TAXPAIDMOSTRECENTFISCALYEAR     23837 non-null  float64       \n",
      " 53  TAXPAIDPRIORFISCALYEAR          23837 non-null  float64       \n",
      " 54  NETINCOMEMOSTRECENTFISCALYEAR   23837 non-null  float64       \n",
      " 55  NETINCOMEPRIORFISCALYEAR        23837 non-null  float64       \n",
      " 56  ID                              25436 non-null  int64         \n",
      " 57  ISSUER                          25434 non-null  object        \n",
      " 58  ISSUERSIGNATURE                 25434 non-null  object        \n",
      " 59  ISSUERTITLE                     25434 non-null  object        \n",
      " 60  ISCOISSUER                      13958 non-null  object        \n",
      "dtypes: datetime64[ns](3), float64(28), int64(2), object(28)\n",
      "memory usage: 11.8+ MB\n"
     ]
    }
   ],
   "source": [
    "# check data type\n",
    "df_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902546f8-98b6-4438-81a5-cfbd57fc1a9e",
   "metadata": {},
   "source": [
    "**Drop uneccessary columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83d6bd0e-039f-4fbe-b68d-ae96cb34166c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop PERIOD\n",
    "df_cleaned.drop('PERIOD', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edfe9e8-d74f-4184-817a-8e87280b6ce1",
   "metadata": {},
   "source": [
    "**Filtering**: To only include relevant filings\n",
    "- Only include the following submission forms: C, C/A, C-U, C-W, C/A-W, C-U-W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4353b52-8288-49ae-941d-2198947cd968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include the following submission forms: C, C/A, C-U, C-W, C/A-W, C-U-W\n",
    "submission_types = ['C', 'C/A', 'C-U', 'C-W', 'C/A-W', 'C-U-W']\n",
    "df_filtered = df_cleaned.loc[df_cleaned['SUBMISSION_TYPE'].isin(submission_types), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d5beb1d-9c85-49a1-bb98-4ec8b738f7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25156\\1178657395.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.sort_values(by=['FILE_NUMBER', 'FILING_DATE', 'SUBMISSION_TYPE'], ascending=[True, True, True], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# To oraganize, sort by FILE_NUMBER and FILING_DATE\n",
    "df_filtered.sort_values(by=['FILE_NUMBER', 'FILING_DATE', 'SUBMISSION_TYPE'], ascending=[True, True, True], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b5771-c90c-4b2c-999f-60f0a6045ae4",
   "metadata": {},
   "source": [
    "**Filtering**: To only include relevant filings\n",
    "- This will be completed much later -> Only include CF offerings that start between 2018 Q1 and 2023 Q4\n",
    "- This is completed below -> Only include CF offerings thst end between 2018 Q1 and 2023 Q4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c786abe-0895-4ac6-bda3-6a1af5f7573a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ACCESSION_NUMBER', 'SUBMISSION_TYPE', 'FILING_DATE', 'CIK',\n",
       "       'FILE_NUMBER', 'ISAMENDMENT', 'PROGRESSUPDATE', 'NATUREOFAMENDMENT',\n",
       "       'NAMEOFISSUER', 'LEGALSTATUSFORM', 'LEGALSTATUSOTHERDESC',\n",
       "       'JURISDICTIONORGANIZATION', 'DATEINCORPORATION', 'STREET1', 'STREET2',\n",
       "       'CITY', 'STATEORCOUNTRY', 'ZIPCODE', 'ISSUERWEBSITE', 'COMPANYNAME',\n",
       "       'COMMISSIONCIK', 'COMMISSIONFILENUMBER', 'CRDNUMBER',\n",
       "       'COMPENSATIONAMOUNT', 'FINANCIALINTEREST', 'SECURITYOFFEREDTYPE',\n",
       "       'SECURITYOFFEREDOTHERDESC', 'NOOFSECURITYOFFERED', 'PRICE',\n",
       "       'PRICEDETERMINATIONMETHOD', 'OFFERINGAMOUNT',\n",
       "       'OVERSUBSCRIPTIONACCEPTED', 'OVERSUBSCRIPTIONALLOCATIONTYPE',\n",
       "       'DESCOVERSUBSCRIPTION', 'MAXIMUMOFFERINGAMOUNT', 'DEADLINEDATE',\n",
       "       'CURRENTEMPLOYEES', 'TOTALASSETMOSTRECENTFISCALYEAR',\n",
       "       'TOTALASSETPRIORFISCALYEAR', 'CASHEQUIMOSTRECENTFISCALYEAR',\n",
       "       'CASHEQUIPRIORFISCALYEAR', 'ACTRECEIVEDRECENTFISCALYEAR',\n",
       "       'ACTRECEIVEDPRIORFISCALYEAR', 'SHORTTERMDEBTMRECENTFISCALYEAR',\n",
       "       'SHORTTERMDEBTPRIORFISCALYEAR', 'LONGTERMDEBTRECENTFISCALYEAR',\n",
       "       'LONGTERMDEBTPRIORFISCALYEAR', 'REVENUEMOSTRECENTFISCALYEAR',\n",
       "       'REVENUEPRIORFISCALYEAR', 'COSTGOODSSOLDRECENTFISCALYEAR',\n",
       "       'COSTGOODSSOLDPRIORFISCALYEAR', 'TAXPAIDMOSTRECENTFISCALYEAR',\n",
       "       'TAXPAIDPRIORFISCALYEAR', 'NETINCOMEMOSTRECENTFISCALYEAR',\n",
       "       'NETINCOMEPRIORFISCALYEAR', 'ID', 'ISSUER', 'ISSUERSIGNATURE',\n",
       "       'ISSUERTITLE', 'ISCOISSUER'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "56529937-c3af-48ce-9cf0-f9617661e317",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25156\\1883406018.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered['FINALDEADLINEDATE'] = np.nan # store the offering's latest deadline -> to be used later in feature engineering # of CF competitors\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25156\\1883406018.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.loc[df_filtered['FILE_NUMBER'] == file_number, 'FINALDEADLINEDATE'] = deadline\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_25156\\1883406018.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.drop(index=selected.index, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Ensure to only include CF offerings thst end between 2018 Q1 and 2023 Q4\n",
    "all_file_numbers = df_filtered['FILE_NUMBER'].unique() # list of all unique FILE_NUMBER values\n",
    "df_filtered['FINALDEADLINEDATE'] = np.nan # store the offering's latest deadline -> to be used later in feature engineering # of CF competitors\n",
    "\n",
    "for file_number in all_file_numbers:\n",
    "    selected = df_filtered.loc[df_filtered['FILE_NUMBER'] == file_number,:]\n",
    "    \n",
    "    # In the case there are updates on offering terms\n",
    "    if (selected['SUBMISSION_TYPE'] == 'C/A').any():\n",
    "        # Obtain deadline from the latest updated offering terms\n",
    "        deadline = selected.loc[selected['SUBMISSION_TYPE'] == 'C/A', :].iloc[-1,:]['DEADLINEDATE']\n",
    "    # In the case that there is no update on offering terms\n",
    "    else:\n",
    "        # The most up-to-date deadline is from the original Form C submission\n",
    "        deadline = selected.loc[selected['SUBMISSION_TYPE'] == 'C', :].iloc[0,:]['DEADLINEDATE']\n",
    "    \n",
    "    # Drop offerings that ends beyond 2023 Q4\n",
    "    if deadline >= pd.Timestamp('2023-12-30'):\n",
    "        df_filtered.drop(index=selected.index, inplace=True)\n",
    "    \n",
    "    # Store the offering's latest deadline -> to be used later in feature engineering # of CF competitors\n",
    "    df_filtered.loc[df_filtered['FILE_NUMBER'] == file_number, 'FINALDEADLINEDATE'] = deadline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f6d6b9-8a31-48bc-b24a-633a7090d009",
   "metadata": {},
   "source": [
    "**Define Target Variable**\n",
    "- 1 and 0 represents successful and failed CF offering, respectively.\n",
    "- Definition of successful CF offering. There are 2 definitions,\n",
    "    - First definition: In the case that the \\$ amount raised is disclosed (Done)\n",
    "        - Ensure that we look at the last C-U filing for a given FILE_NUMBER\n",
    "        - Obtain the \\$ amount raised from '`PROGRESSUPDATE` using regular expression\n",
    "        - Compare the \\$ amount raised with the `OFFERINGAMOUNT`\n",
    "    - Second definition: In the case that the \\$ amount is not disclosed (Done)\n",
    "        - Ensure that we look at the last C-U filing for a given FILE_NUMBER\n",
    "        - `PROGRESSUPDATE` contains the word '100%' -> use regular expression to allow any symbol between '100' and '%'\n",
    "- Definition of failed CF offering.\n",
    "    - First definition: In the case that C-U form is not provided. (Done)\n",
    "    - Second definition: C-U form is provided, but the disclosed amount raised is 0 or less than the targeted amount. (Done)\n",
    "    - Third definition: If the Form C-W is filed. (Done)\n",
    "        - Those that filed C-W are usually the ones that fail to meet the offering target, and so when C-W is filed, it will be considered as a failed CF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24024e-131a-4413-86a8-10d49700c9bc",
   "metadata": {},
   "source": [
    "Relevant for the `extract_offering_raised` function,\n",
    "- If there are more than one \\$ amount mentioned, pick the largest one. For example, \"On November 30th 2019 Aapoon, Inc raised `\\$102.100` against the goal of raising `\\$100,000`\".\n",
    "    - Examples:\n",
    "        - \"The issuer closed on $19,300 of SAFEs. The funding portal was granted $380 of securities as compensation. Raise Green was granted $1,000 in additional compensation for a Form C/A Material Change.\"\n",
    "\n",
    "Regex rule for if the $ amount raised was not disclosed -> relevant for the `extract_text_outcome` function. 3 Options:\n",
    "- Success\n",
    "    - What should be in the regex?\n",
    "        - Make sure to lower case all the letters in the PROGRESSUPDATE\n",
    "        - There is no \"not\" word in the description\n",
    "        - \"success\", \"successfully\"\n",
    "        - \"100%\", \"100 percent\"\n",
    "        - \"reach\", \"reached\"\n",
    "        - \"minimum\"\n",
    "        - \"met\"\n",
    "        - \"full\", \"fully\"\n",
    "        - \"exceed\", \"exceeded\"\n",
    "    - Examples:\n",
    "        - \"The Offering closed successfully on 8/06/2021, having received investment commitments of 90,040.00\"\n",
    "        - \"Initial closed updated for a successfully closed offering.\"\n",
    "        - \"100% of minimum target reached\"\n",
    "        - \"Minimum Funding Goal has been met.\"\n",
    "        - \"Minimum target amount raised\"\n",
    "        - \"100% of target amount raised by target date. Offering closed\"\n",
    "        - \"100% of Target Goal reached Target date reached and offering closed\"\n",
    "        - \"Project fully funded.\"\n",
    "        - \"Max target reached.\"\n",
    "        - \"Project has reached the Target Offering Amount withing the Offering Period.\"\n",
    "        - \"Reached the target and almost the max offering amount.\"\n",
    "        - \"Target Amount in the Form C reached, Issuer intends to engage in initial close and collection of committed funds and then a subsequent re-opening of its offering.\"\n",
    "        - \"Luma Resources LLC is has received investment committments and has funds in escrow that are in excess of 50 percent and 100 percent of the target offering amount.\"\n",
    "        - \"The offer exceeded its original goal with 220 investors 349,000 shares sold. The will continue its raise until it reach 107,000.\"\n",
    "        \n",
    "- Fail\n",
    "    - What should be in the regex?\n",
    "        - Make sure to lower case all the letters in the PROGRESSUPDATE\n",
    "        - \"fail\", \"failed\"\n",
    "        - \"return\", \"returned\"\n",
    "        - \"refund\", \"refunded\"\n",
    "        - \"The company did not have any sales\"\n",
    "        - \"No securities were sold in this offering\"\n",
    "        - \"No investments were sold\"\n",
    "        - \"did not sell\"\n",
    "        - \"not\"\n",
    "        - \"no sales\"\n",
    "        - The word \"50%\" is in the desription, but the word \"100%\" is not\n",
    "    - Examples:\n",
    "        - \"End of offering. Issuer failed to reach target amount and investor funds are being returned by the escrow agent.\"\n",
    "        - \"THE OFFERING DID NOT MEET TARGET; FUNDS BEING RETURNED TO INVESTORS.\"\n",
    "        - \"The company did not have any sales in this offering.\"\n",
    "        - \"50% of target amount raised.\"\n",
    "        - \"The offering did not meet its target; funds being returned to investors.\"\n",
    "        - \"The issuer failed to meet the minimum target amount in the offering. All funds committed by investors have been refunded.\"\n",
    "        - \"The raise did not meet its minimum target amount and thus was not successful. Investor transaction commitments are being canceled and monies are being refunded.\"\n",
    "        - \"No securities were sold in this offering.\"\n",
    "        - \"The Company did not sell any securities under Regulation CF\"\n",
    "        - \"Offering closed.  Target amount not reached\" -> this appears quite often\n",
    "        - \"Offering closed.  Target amount was not reahed\" -> yes it is a typo\n",
    "        - \"The issuer did not sell any securities or raise any capital in this offering prior to its deadline.\"\n",
    "        - \"Update filing for termination of offering.  No investments were sold in this offering.\"\n",
    "        - \"The offering is closed and the company has made no sales.\"\n",
    "        - \"The raise did not meet its minimum target amount and thus was not successful. Investor transaction commitments are being canceled and monies are being refunded.\"\n",
    "        \n",
    "- Remove from dataset\n",
    "    - What should be in the regex?\n",
    "        - Make sure to lower case all the letters in the PROGRESSUPDATE\n",
    "    - Examples:\n",
    "        - \"Issuer is launching campaign with StartEngine\"\n",
    "        - \"To disclose related material events; which originated on April 30, 2021 when the Issuer's Portal terminated trading as noted in the CUW.  The Issuer has replaced the former Portal with truCrowd and hereby amends the CUW file to continue the Offering.\"\n",
    "        - \"The Form C-U is amended to list the correct offering deadline of July 21, 2020.\"\n",
    "        - \"Three43 Inc. has added a new video (Video 2) further describing their product.\"\n",
    "        - \"The Offering closed on 9/21/2023 due to the untimely closing, withdrawal, of the Broker Dealer of Record.\"\n",
    "        - \"Offer ended on 4/01/2019 with 138 investors.\"\n",
    "        - \"The offering closed December 31,2021 \"\n",
    "        - \"Completion of Offering\"\n",
    "    \n",
    "- Manually classify -> I decided to just remove these data points\n",
    "    - Examples:\n",
    "        - \"At the close of the offering, the issuer closed on 106,787.52 and 228,860 number of securities.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9363566f-c188-418a-b8d2-681669274858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_offering_raised(progress_update_text):\n",
    "    '''\n",
    "    Uses regular expression to extract the $ amount of offering raised from PROGRESSUPDATE column \n",
    "    '''    \n",
    "    # Use regex to extract amount raised\n",
    "    pattern = r'\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d{2})?' # r'\\$\\d{1,3}(?:,\\d{3})*(?:\\.\\d{1,2})?|\\$\\d+(?:\\.\\d{1,2})?'\n",
    "    matches = re.findall(pattern, progress_update_text)\n",
    "    \n",
    "    # In the case that no $ amount raised was disclosed\n",
    "    if not matches:\n",
    "        return np.nan\n",
    "    \n",
    "    # convert from string to float\n",
    "    amount_raised = []\n",
    "    for match in matches:\n",
    "        amount_raised.append(float(match[1:].replace(',', '')))\n",
    "    \n",
    "    # in the case that there are multiple $ figures, pick the largest one\n",
    "    result = max(amount_raised)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61f016b2-630b-40ee-ac11-322ef9d937bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_outcome(progress_update_text):\n",
    "    '''\n",
    "    Use regex to extract on whether the CF is a success, failure, or should be omitted from the dataset, in the case that\n",
    "    $ amount raised was not disclosed in the latest C-U filing.\n",
    "    Returns:\n",
    "        'success' -> Successful CF\n",
    "        'fail' -> Unsuccessful CF\n",
    "        'uncertain' -> Not classified (to be removed from the training dataset)\n",
    "    '''\n",
    "    text = progress_update_text.lower()\n",
    "\n",
    "    # Failure patterns\n",
    "    fail_patterns = [\n",
    "        r'\\bfail(?:ed)?\\b',\n",
    "        r'\\breturn(?:ed)?\\b',\n",
    "        r'\\brefund(?:ed)?\\b',\n",
    "        r'did not have any sales',\n",
    "        r'no securities were sold in this offering',\n",
    "        r'no investments were sold',\n",
    "        r'did not sell',\n",
    "        r'\\bno sales\\b',\n",
    "        r'\\bnot\\b'\n",
    "    ]\n",
    "    \n",
    "    # Special case: 50% present without 100%\n",
    "    if re.findall(r'\\b50\\s*%|\\b50\\s*percent\\b', text) and not re.findall(r'\\b100\\s*%|\\b100\\s*percent\\b', text):\n",
    "        return 'fail'\n",
    "    \n",
    "    if any(re.findall(p, text) for p in fail_patterns):\n",
    "        return 'fail'\n",
    "    \n",
    "    # Success patterns\n",
    "    # Must not contain the word \"not\"\n",
    "    if 'not' in text:\n",
    "        return 'fail'\n",
    "\n",
    "    success_patterns = [\n",
    "        r'\\bsuccess(?:fully)?\\b',\n",
    "        r'\\b100\\s*%|\\b100\\s*percent\\b',\n",
    "        r'\\breach(?:ed)?\\b',\n",
    "        r'\\bminimum\\b',\n",
    "        r'\\bmet\\b',\n",
    "        r'\\bfull(?:y)?\\b',\n",
    "        r'\\bexceed(?:ed)?\\b'\n",
    "    ]\n",
    "\n",
    "    if any(re.findall(p, text) for p in success_patterns):\n",
    "        return 'success'    \n",
    "\n",
    "    # If no match\n",
    "    return 'uncertain'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dadef62-5d6d-4ee1-8bd9-9105bc8c82ee",
   "metadata": {},
   "source": [
    "Note that the cell below, I am defining the amount raised and final target offering amount, which will be used later to define target variable in the next 2 cell.\n",
    "\n",
    "The general rule in defining the amount raised is as follows:\n",
    "- If the \\$ amount raised is disclosed, that will be used as the amount raised.\n",
    "- If the \\$ amount raised is not disclosed, but the progress update describe whether the outcome is success or failure: (i) if it indicates success, the amount raised is assumed to be equal to the final target offering amount; (ii) if it indicates failure, the amount raised is assumed to be zero\n",
    "- If there is no C-U filed, the amount raised is assume to be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59428a9b-5b27-43c1-a057-8388dd7f4ad2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_file_numbers = df_filtered['FILE_NUMBER'].unique() # list of all unique FILE_NUMBER values\n",
    "\n",
    "# Store updated dataset\n",
    "df_filtered_2 = df_filtered.copy()\n",
    "df_filtered_2['FINALOFFERINGTARGET'] = np.nan # create new column to store the final offering target amount\n",
    "df_filtered_2['AMOUNTRAISED'] = np.nan # create new column to store $ amount raised\n",
    "df_filtered_2['Y'] = np.nan # create a new column to store the target variable\n",
    "\n",
    "# Copmute total amount raised\n",
    "for file_number in all_file_numbers:\n",
    "    dropped_idx = []\n",
    "    # Select the offering\n",
    "    selected = df_filtered.loc[df_filtered['FILE_NUMBER'] == file_number, :]\n",
    "    \n",
    "    # Extract the final progress update in the latest C-U filing, if any\n",
    "    progress_update = np.nan\n",
    "    for i in range(-1, len(selected)*-1 - 1, -1):\n",
    "        if selected.iloc[i, :]['SUBMISSION_TYPE'] == 'C-U':\n",
    "            progress_update = selected.iloc[i, :]['PROGRESSUPDATE']\n",
    "            break\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "    # extract the latest target offering amount\n",
    "    final_target = np.nan\n",
    "    for i in range(-1, len(selected)*-1 - 1, -1):\n",
    "        if pd.isna(final_target):\n",
    "            final_target = selected.iloc[i, :]['OFFERINGAMOUNT']\n",
    "        else:\n",
    "            break\n",
    "    df_filtered_2.loc[selected.index, 'FINALOFFERINGTARGET'] = final_target\n",
    "            \n",
    "    # if there is no C-U filing, i.e. no progress update\n",
    "    if pd.isna(progress_update):\n",
    "        df_filtered_2.loc[selected.index, 'AMOUNTRAISED'] = 0 # set amount raised to 0\n",
    "        continue\n",
    "    # otherwise, extract the amount raised from progress_update\n",
    "    else:\n",
    "        amount_raised = extract_offering_raised(progress_update)\n",
    "\n",
    "    # if `amount_raised` is not null\n",
    "    if not pd.isna(amount_raised):\n",
    "        # update the AMOUNTRAISED column\n",
    "        df_filtered_2.loc[selected.index, 'AMOUNTRAISED'] = amount_raised\n",
    "    # if `amount_raised` is null\n",
    "    else:\n",
    "        regex_classification = extract_text_outcome(progress_update)\n",
    "        # set AMOUNTRAISED equal to the final_target amount\n",
    "        if regex_classification == 'success':\n",
    "            df_filtered_2.loc[selected.index, 'AMOUNTRAISED'] = final_target\n",
    "        # set AMOUNTRAISED to 0\n",
    "        elif regex_classification == 'fail':\n",
    "            df_filtered_2.loc[selected.index, 'AMOUNTRAISED'] = 0\n",
    "        # remove unclassified datapoints\n",
    "        else:\n",
    "            # print(progress_update) ##############################\n",
    "            df_filtered_2.drop(index=selected.index, inplace=True)\n",
    "            dropped_idx = selected.index.to_list()\n",
    "    \n",
    "    # Special case: When C-W or C/A-W is filed\n",
    "    for i in range(-1, len(selected)*-1 - 1, -1):\n",
    "        if selected.iloc[i, :]['SUBMISSION_TYPE'] == 'C/A-W':\n",
    "            break\n",
    "        elif not dropped_idx and selected.iloc[i, :]['SUBMISSION_TYPE'] == 'C-W':\n",
    "            df_filtered_2.loc[selected.index, 'AMOUNTRAISED'] = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db35c2-3beb-415d-917a-565383fa44c5",
   "metadata": {},
   "source": [
    "Before we continue, I noticed that there is an entry error of file number '020-32053', where it has no information on OFFERINGAMOUNT, so we are going to remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b61bc15-000a-4b98-8fd2-09c788a2aa03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ACCESSION_NUMBER</th>\n",
       "      <th>SUBMISSION_TYPE</th>\n",
       "      <th>FILING_DATE</th>\n",
       "      <th>CIK</th>\n",
       "      <th>FILE_NUMBER</th>\n",
       "      <th>PROGRESSUPDATE</th>\n",
       "      <th>NATUREOFAMENDMENT</th>\n",
       "      <th>DEADLINEDATE</th>\n",
       "      <th>OFFERINGAMOUNT</th>\n",
       "      <th>AMOUNTRAISED</th>\n",
       "      <th>FINALOFFERINGTARGET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20627</th>\n",
       "      <td>0001376474-23-000168</td>\n",
       "      <td>C</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>020-32053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20612</th>\n",
       "      <td>0001376474-23-000174</td>\n",
       "      <td>C-W</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>020-32053</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           ACCESSION_NUMBER SUBMISSION_TYPE FILING_DATE  CIK FILE_NUMBER  \\\n",
       "20627  0001376474-23-000168               C  2023-03-27  NaN   020-32053   \n",
       "20612  0001376474-23-000174             C-W  2023-03-28  NaN   020-32053   \n",
       "\n",
       "      PROGRESSUPDATE NATUREOFAMENDMENT DEADLINEDATE  OFFERINGAMOUNT  \\\n",
       "20627            NaN               NaN          NaT             NaN   \n",
       "20612            NaN               NaN          NaT             NaN   \n",
       "\n",
       "       AMOUNTRAISED  FINALOFFERINGTARGET  \n",
       "20627           0.0                  NaN  \n",
       "20612           0.0                  NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print offering file number '020-32053'\n",
    "cols = ['ACCESSION_NUMBER', 'SUBMISSION_TYPE', 'FILING_DATE', 'CIK', 'FILE_NUMBER', 'PROGRESSUPDATE', 'NATUREOFAMENDMENT', 'DEADLINEDATE', 'OFFERINGAMOUNT', 'AMOUNTRAISED', 'FINALOFFERINGTARGET']\n",
    "anomaly_cf = df_filtered_2.loc[df_filtered_2['FILE_NUMBER'] == '020-32053', cols]\n",
    "display(anomaly_cf)\n",
    "\n",
    "# drop the anomaly cf\n",
    "df_filtered_2.drop(index=anomaly_cf.index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbc3911-aa00-489f-a21d-9654d9b1b0cf",
   "metadata": {},
   "source": [
    "Now, we can use the `AMOUNTRAISED` and `FINALOFFERINGTARGET` that we defined previously to determine whether the fundraising is successful or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e31072ba-7458-43db-890a-541adb87b3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "df_filtered_2['Y'] = (df_filtered_2['AMOUNTRAISED'] >= df_filtered_2['FINALOFFERINGTARGET']).replace({True:1, False:0})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadb3c3-6d3f-4f08-a3ad-00b61d71a5da",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "- Note that we are creating a ML model to predict the success/failure of CF offering at the start date of the CF offering. Therefore, for the features, we will be using information up to the start date of the CF offerring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50e20577-49bd-48fc-8711-1e632bc27497",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df = pd.DataFrame({'FILE_NUMBER': df_filtered_2['FILE_NUMBER'].unique()})\n",
    "ml_df.set_index('FILE_NUMBER', inplace=True)\n",
    "\n",
    "# Add the target variable\n",
    "ml_df['Y'] = np.nan\n",
    "for num in df_filtered_2['FILE_NUMBER'].unique():\n",
    "    selected = df_filtered_2.loc[df_filtered_2['FILE_NUMBER'] == num, :]\n",
    "    for i in range(len(selected)):\n",
    "        if selected.iloc[i, :]['SUBMISSION_TYPE'] == 'C':\n",
    "            ml_df.loc[num, 'Y'] = selected.iloc[i, :]['Y']\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff09ea3-0162-4095-8680-244cfa59ca9c",
   "metadata": {},
   "source": [
    "### Transfer Features from `df_filtered_2` to `ml_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63125a12-4bab-4ed6-b5fc-f7c6e475b28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_latest_variable(col_df_filtered, col_ml_df):\n",
    "    '''\n",
    "    This function is used to extract the latest information on a variable of a CF offering, transfering the information from\n",
    "    the df_filtered_2 to ml_df.\n",
    "    Parameters\n",
    "    ----------\n",
    "    col_df_filtered: str\n",
    "        The feature name from the df_filtered_2 dataset that you want to extract.\n",
    "    col_ml_df: str\n",
    "        The feature name that you want your feature to be called in ml_df.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    It does not return anything. It automatically updates the ml_df dataset.\n",
    "    '''\n",
    "    all_file_numbers = ml_df.index.unique() # list of all unique FILE_NUMBER values\n",
    "    ml_df[col_ml_df] = np.nan # store the offering's latest information\n",
    "\n",
    "    for file_number in all_file_numbers:\n",
    "        selected = df_filtered_2.loc[df_filtered_2['FILE_NUMBER'] == file_number,:]\n",
    "\n",
    "        # In the case there are updates on offering terms\n",
    "        if (selected['SUBMISSION_TYPE'] == 'C/A').any():\n",
    "            # Obtain deadline from the latest updated offering terms\n",
    "            var = selected.loc[selected['SUBMISSION_TYPE'] == 'C/A', :].iloc[-1,:][col_df_filtered]\n",
    "        # In the case that there is no update on offering terms\n",
    "        else:\n",
    "            # The most up-to-date deadline is from the original Form C submission\n",
    "            var = selected.loc[selected['SUBMISSION_TYPE'] == 'C', :].iloc[0,:][col_df_filtered]\n",
    "\n",
    "        # Store the offering's latest deadline -> to be used later in feature engineering # of CF competitors\n",
    "        ml_df.loc[file_number, col_ml_df] = var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43bf35b1-96dd-4057-8178-933ca503f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_filetered_df_to_ml_df(col_df_filtered, col_ml_df):\n",
    "    '''\n",
    "    This function is used to extract a feature from df_filtered_2 to ml_df. More specifically, it extracts features\n",
    "    from the intial form C submission in the df_filtered_2.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    col_df_filtered: str\n",
    "        The feature name from the df_filtered_2 dataset that you want to extract.\n",
    "    col_ml_df: str\n",
    "        The feature name that you want your feature to be called in ml_df.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    It does not return anything. It automatically updates the ml_df dataset.\n",
    "    '''\n",
    "    ml_df[col_ml_df] = np.nan\n",
    "    for num in df_filtered_2['FILE_NUMBER'].unique():\n",
    "        selected = df_filtered_2.loc[df_filtered_2['FILE_NUMBER'] == num, :]\n",
    "        for i in range(len(selected)):\n",
    "            if selected.iloc[i, :]['SUBMISSION_TYPE'] == 'C':\n",
    "                ml_df.loc[num, col_ml_df] = selected.iloc[i, :][col_df_filtered]\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9ddc8e-1350-4691-82c7-7c918c9be8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract revenue figure from the most recent fiscal year\n",
    "extract_filetered_df_to_ml_df('REVENUEMOSTRECENTFISCALYEAR', 'REVENUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5052b5d2-23be-4396-a45f-73aae083ba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract revenue figure from the previous fiscal year\n",
    "extract_filetered_df_to_ml_df('REVENUEPRIORFISCALYEAR', 'PREVIOUSREVENUE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f6d1261-b4d8-4ddb-a0bd-f63829ed44ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract total asset from the current fiscal year\n",
    "extract_filetered_df_to_ml_df('TOTALASSETMOSTRECENTFISCALYEAR', 'ASSET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b314713f-65d3-40c2-a64e-ed129fd21c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cash and cash-equivalents from the current fiscal year\n",
    "extract_filetered_df_to_ml_df('CASHEQUIMOSTRECENTFISCALYEAR', 'CASH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b464883-fd48-4092-b1a7-3749db0da67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract long-term debt from the current fiscal year\n",
    "extract_filetered_df_to_ml_df('LONGTERMDEBTRECENTFISCALYEAR', 'LONGTERMDEBT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "543a8cdc-af03-48c7-be78-f2f1484c21e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract net income from the current fiscal year\n",
    "extract_filetered_df_to_ml_df('NETINCOMEMOSTRECENTFISCALYEAR', 'NETINCOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb530600-35f9-4d6f-9455-b656c86f6c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract short-term debt from the current fiscal year\n",
    "extract_filetered_df_to_ml_df('SHORTTERMDEBTMRECENTFISCALYEAR', 'SHORTTERMDEBT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7a0fb1e3-e86d-43e5-b483-a9d79bae101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Acc. Rec. from the current fiscal year\n",
    "extract_filetered_df_to_ml_df('ACTRECEIVEDRECENTFISCALYEAR', 'ACCREC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8c8086d5-e7ad-4013-9358-7d63b27a609b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract COGSfrom the current fiscal year\n",
    "extract_filetered_df_to_ml_df('COSTGOODSSOLDRECENTFISCALYEAR', 'COGS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b2f8c5e1-1e05-48cf-9024-d6f32505b425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add minimum investiment size feature\n",
    "extract_filetered_df_to_ml_df('PRICE', 'MINIMUMSIZE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "71213bbc-5a81-4a2a-9896-34f30507410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the commission in text format\n",
    "extract_filetered_df_to_ml_df('COMPENSATIONAMOUNT', 'TEXT_RAISEDCOMMISSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6e9fa6a8-9c8a-4db6-8dbc-a121769552f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the commission in text format\n",
    "extract_filetered_df_to_ml_df('FINANCIALINTEREST', 'TEXT_EQUITYCOMMISSION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f4a4892b-3d22-443d-a819-836b4cfcbb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the platform ID\n",
    "extract_filetered_df_to_ml_df('COMMISSIONCIK', 'COMMISSIONCIK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca0c3600-d54f-4fc7-98e6-f91ac1d2c34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the offering start date\n",
    "extract_filetered_df_to_ml_df('FILING_DATE', 'FILING_DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ccb59df3-abce-4f93-85cc-e1e93427a017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the offering deadline\n",
    "extract_filetered_df_to_ml_df('FINALDEADLINEDATE', 'FINALDEADLINEDATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6bcc41e0-920a-490c-9e43-7318fc5dcfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the target offering amount\n",
    "extract_latest_variable('OFFERINGAMOUNT', 'FINALOFFERINGAMOUNT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ae91bd55-0faf-4073-a6d1-e315c024d3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the type of security offered in the CF\n",
    "extract_latest_variable('SECURITYOFFEREDTYPE', 'SECURITYOFFEREDTYPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d676116c-3857-4fe3-ba4a-fd6a137c05ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain further description for \"Others\" security type\n",
    "extract_latest_variable('SECURITYOFFEREDOTHERDESC', 'SECURITYOFFEREDOTHERDESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0ff0fb21-d049-4c6f-bf44-f48c88d816b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PRICEDETERMINATIONMETHOD\n",
    "extract_filetered_df_to_ml_df('PRICEDETERMINATIONMETHOD', 'PRICEDETERMINATIONMETHOD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d07ee440-92cb-4a98-b60b-18ec9447c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the date when the company was found\n",
    "extract_filetered_df_to_ml_df('DATEINCORPORATION', 'DATEINCORPORATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41994147-7ccf-43de-88bb-bd523576e430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the issuer ID\n",
    "extract_filetered_df_to_ml_df('CIK', 'CIK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "779a7b0c-f934-46c6-a3b1-0f189d35475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the issuing person's name\n",
    "extract_filetered_df_to_ml_df('ISSUERSIGNATURE', 'ISSUERSIGNATURE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0688a856-dbb8-4e55-bba5-bc64ae33b868",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the number of employees of the issuer\n",
    "extract_filetered_df_to_ml_df('CURRENTEMPLOYEES', 'CURRENTEMPLOYEES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e245419a-9c99-450a-a96e-8c472e12fd3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the organization form of the issuer\n",
    "extract_filetered_df_to_ml_df('LEGALSTATUSFORM', 'LEGALSTATUSFORM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "14af04fb-3599-4aeb-8fb7-e2749dede40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the organization form description of the issuer\n",
    "extract_filetered_df_to_ml_df('LEGALSTATUSOTHERDESC', 'LEGALSTATUSOTHERDESC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0d76d8cc-b2b7-4c78-98fd-69ed25d800b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the state in which the company is registered / incorporated\n",
    "extract_filetered_df_to_ml_df('JURISDICTIONORGANIZATION', 'JURISDICTIONORGANIZATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5dddc3ee-15e9-4513-a77f-fa31331ba9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the state in which the company is currently operating\n",
    "extract_filetered_df_to_ml_df('STATEORCOUNTRY', 'STATEORCOUNTRY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2c308e-be68-46d4-bd13-640fcc0aa6c9",
   "metadata": {},
   "source": [
    "### Cleaning The ml_df Dataset (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a768cf01-3f55-41c4-ba5f-c2fd2d62ab00",
   "metadata": {},
   "source": [
    "The following will be performed:\n",
    "- Exclude data points where it reports 0 cash.\n",
    "- Exclude data points where it simulateneosly reports 0 revenue and non-zero COGS.\n",
    "- Exclude data points where it simulateneosly reports non-zero revenue and 0 COGS.\n",
    "- Exclude data points where it simultaneously reports 0 revenue, 0 COGS, 0 net income, and the company has employees on record (which they would have to pay, i.e. wages expense).\n",
    "- Only include data points where the issuer operates in one of the 50 US states, excluding  regions such as DC (DISTRICT OF COLUMBIA), PR (PUERTO RICO), VI (VIRGIN ISLANDS), etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cbdf288e-e211-449a-8c76-01596e3582b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save ml_df just in case\n",
    "ml_df_copy = ml_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "eb9c31ca-9d90-4fe4-8e98-67fc31c79958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to exclude companies that have 0 cash\n",
    "mask = (ml_df['CASH'] == 0)\n",
    "ml_df = ml_df.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "1e8b2dd6-f71f-4f0f-a182-60607e777e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to exclude companies that simulateneosly reports 0 revenue and non-zero COGS.\n",
    "mask = (ml_df['REVENUE'] == 0) & (ml_df['COGS'] != 0)\n",
    "ml_df = ml_df.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "7de682cd-9813-49cc-9a66-81817f3c1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to exclude companies that simulateneosly reports non-zero revenue and 0 COGS.\n",
    "mask = (ml_df['REVENUE'] != 0) & (ml_df['COGS'] == 0)\n",
    "ml_df = ml_df.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "287c492e-fb79-4c78-92d9-835edcb72df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to exclude companies that simultaneously reports 0 revenue, 0 COGS, 0 net income, and the company has employees on record\n",
    "mask = (ml_df['REVENUE'] == 0) & (ml_df['COGS'] == 0) & (ml_df['NETINCOME'] == 0)\n",
    "ml_df = ml_df.loc[~mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "a574e34a-c944-421d-a99d-8c4b1749ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to only include companies that are operating in the USA\n",
    "us_states = {\n",
    "    'AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','IA','ID','IL','IN',\n",
    "    'KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH',\n",
    "    'NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA',\n",
    "    'VT','WA','WI','WV','WY'\n",
    "}\n",
    "\n",
    "mask = ml_df['STATEORCOUNTRY'].isin(us_states)\n",
    "ml_df = ml_df.loc[mask, :].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f039800-8750-4879-88d2-aa536582412d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Historical Financial Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ffe468-b8dd-41ec-bd7d-a5a1ee018c7e",
   "metadata": {},
   "source": [
    "Revenue Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "63a5abd1-816a-4faa-86f5-76766c947ef0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['REVENUE'] = ml_df['REVENUE'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3cd640-4944-4651-849f-eb14a70325a8",
   "metadata": {},
   "source": [
    "Revenue Growth Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "dcb9aab6-4c67-4dd3-81d6-7ef1a945cf06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['PREVIOUSREVENUE'] = ml_df['PREVIOUSREVENUE'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "3322b635-5de0-4d15-acef-6103bc045392",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Revenue Log Growth\n",
    "ml_df['REVENUEGROWTH'] = np.log(ml_df['REVENUE'] + 1) - np.log(ml_df['PREVIOUSREVENUE'] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e931d1-0da7-49fa-88bc-e40909acce5d",
   "metadata": {},
   "source": [
    "Asset Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "9c904924-dd58-4358-99e2-26d3a26d1cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['ASSET'] = ml_df['ASSET'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d418df2-016b-4147-a8ae-f9ac54e9350a",
   "metadata": {},
   "source": [
    "Cash Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "547e162c-3455-475f-a681-8478b7493414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['CASH'] = ml_df['CASH'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e72a62-1411-40d1-9b10-266ccdb1e8d8",
   "metadata": {},
   "source": [
    "Long-Term Debt Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "b52b58b9-1d99-4506-8bb3-af2f44eb5fd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['LONGTERMDEBT'] = ml_df['LONGTERMDEBT'].abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e91a3-d7ee-466b-86cd-135eb2ade6af",
   "metadata": {},
   "source": [
    "Debt-to-Asset Ratio Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "eb16331d-c91e-4692-8778-520c3ea64ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['SHORTTERMDEBT'] = ml_df['SHORTTERMDEBT'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "9f06046a-a510-42dc-8fbc-97e352d42f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute debt-to-asset ratio\n",
    "ml_df['DEBTTOASSET'] = np.arcsinh((ml_df['LONGTERMDEBT'] + ml_df['SHORTTERMDEBT']) / (ml_df['ASSET'] + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e5fc7a-7501-415d-baf0-d62071efc555",
   "metadata": {},
   "source": [
    "Working Capital Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "e657de96-443b-4804-bbc9-bb6537790014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['ACCREC'] = ml_df['ACCREC'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4b959372-e519-4207-882e-e03c995f24a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute working capital\n",
    "ml_df['WORKINGCAPITAL'] = ml_df['CASH'] + ml_df['ACCREC'] - ml_df['SHORTTERMDEBT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae602dc4-1332-45fa-a9be-3aa9f5f581ba",
   "metadata": {},
   "source": [
    "Return on Asset (ROA) Ratio Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3f184b37-73d6-4c2f-a8eb-557a343135bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROA\n",
    "ml_df['ROA'] = np.arcsinh(ml_df['NETINCOME'] / (ml_df['ASSET'] + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263a258d-6d47-45e4-b29b-a2d65e708c4b",
   "metadata": {},
   "source": [
    "Gross Profit Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "15025320-0129-4d96-ae64-9fe908bae735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Correct any misinputs\n",
    "ml_df['COGS'] = ml_df['COGS'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "02f9e95a-9748-409a-86a0-08a63461b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gross profit\n",
    "ml_df['GROSSPROFIT'] = ml_df['REVENUE'] - ml_df['COGS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c7c6d7-fac2-428f-bd29-b448d92f0d9a",
   "metadata": {},
   "source": [
    "Gross Margin Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "ffd346a0-06d0-479c-bafe-b3eb33bdbfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gross margin\n",
    "ml_df['GROSSMARGIN'] = np.arcsinh((ml_df['REVENUE'] - ml_df['COGS']) / (ml_df['REVENUE'] + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6e9407-2cdb-4d2c-8f76-cde9ab240869",
   "metadata": {},
   "source": [
    "Net Profit Margin Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cd22d279-b459-41a1-8d6f-b92dded0af1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute gross margin\n",
    "ml_df['NETPROFITMARGIN'] = np.arcsinh(ml_df['NETINCOME'] / (ml_df['REVENUE'] + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b168e-b38e-48cd-abfb-9cffbb35a5a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Crowdfunding Characteristics Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78506151-8b37-46d9-b782-4ced149b1257",
   "metadata": {},
   "source": [
    "Platform Fee (commission based on amount raised) Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e6c095-8cac-4dab-bd5f-48c63b70b646",
   "metadata": {},
   "source": [
    "After Platform % Fee based on amount raised has been extracted, we use the following rule set:\n",
    "- If there is only a single percentage, we just use that as the Platform % Fee based on amount raised.\n",
    "- If there are more than three commissions extracted: For clarity, look at the following example, given \"Applied at marginal rate based upon amount of total offering: up to $50,000 = 8.0%, $50,001 - $100,000 = 7.0%, $100,001+ = 6.0%. $250 posting fee. 2.85% investment fee capped at $37.25.\", we would have extracted [8.0, 7.0, 6.0, 2.85]. Since know that we should average the [8.0, 7.0, 6.0] and add the 2.85 to the average., we take the following steps,\n",
    "    - Order the list in descending order.\n",
    "    - If the difference between consecutive sequence is less than or equal to 2, we average them; Otherwise, we will add the number to the average."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f48f66-1c67-42c1-8703-139c3fb33d94",
   "metadata": {
    "tags": []
   },
   "source": [
    "Regex for COMPENSATIONAMOUNT Examples:\n",
    "\n",
    "Contains % commission only\n",
    "- \"The intermediary will get paid a success fee of 9%.\"\n",
    "- \"The intermediary will get paid a success fee of 7%\"\n",
    "- \"6 percent\"\n",
    "- \"7 - 13 percent\"\n",
    "- \"7.0 percent\"\n",
    "- \"MainVest will be paid 6.0% of the final offering amount, upon the successful completion of the offering. MainVest does not receive compensation if the offering does not succeed.\"\n",
    "- \"<p>6.5% of the offering amount upon a successful fundraise, and be entitled to reimbursement for out-of-pocket third party expenses it pays or incurs on behalf of the Issuer in connection with the offering.</p>\"\n",
    "- \"At the conclusion of the offering, the issuer shall pay a fee of six percent (6%) of the amount raised in the offering to the Intermediary.\"\n",
    "- \"7.5% of the offering amount upon a successful fundraise, and be entitled to reimbursement for out-of-pocket third party expenses it pays or incurs on behalf of the Issuer in connection with the offering.\"\n",
    "- \"MainVest will be paid Three (3) Percent of the amount of the Offering raised by \"In-Network Users\" of the Platform plus Nine (9) Percent of the amount of the Offering raised by all other investors.\"\n",
    "- \"MainVest will be paid 6.0% of the final offering amount, upon the successful completion of the offering. MainVest does not receive compensation if the offering does not succeed.\"\n",
    "\n",
    "Tiered fee structure\n",
    "- \"Applied at marginal rate based upon amount of total offering: up to$50,000 = 8.0%, $50,001 - $100,000 = 7.0%, $100,001+ = 6.0%. $250 posting fee. 2.85% investment fee capped at $37.25.\"\n",
    "- \"The intermediary collects a 5% fee on the first $1 million raised. 3% of anything raised over $1 million\"\n",
    "- 10% of the total Offering Amount; provided only 5% is charged on investors introduced by the issuer.\n",
    "- \"MainVest will be paid Three (3) Percent of the amount of the Offering raised by First Time Users of the Platform plus Nine (9) Percent of the amount of the Offering raised by all other investors.\"\n",
    "- \"A % of the total Offering Amount (10% for up to $250,000, 9% for more than $250,000 and $500,000 or less); provided only 5% is charged on investors introduced by the issuer.\"\n",
    "- \"A % of the total Offering Amount (10% for up to $250,000, 9% for more than $250,000 and $500,000 or less, 8% for more than $500,000 and $750,000 or less, and 7% for more than $750,000); provided only 5% is charged on investors introduced by the issuer.\"\n",
    "\n",
    "Interesting cases (Done)\n",
    "- \"The issuer will not owe a commission, whether cash or otherwise, to the Intermediary at the conclusion of the Offering.\"\n",
    "- \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "31d5f3b1-ebcc-42c8-a49d-ebe68a3266ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_commission_fee(commission_text):\n",
    "    '''\n",
    "    This function uses regex to extract % commission from the amount raised. \n",
    "    '''\n",
    "    commission_text = commission_text.lower()\n",
    "    \n",
    "    pattern = re.compile(\n",
    "        r\"\"\"(?xi)                              # \n",
    "        (?:                                    # ┌── two main alternatives ──┐\n",
    "           \\d+(?:\\.\\d+)?                       # │  “6” or “6.5” or “7.0”    │\n",
    "           (?:\\s*[-–]\\s*\\d+(?:\\.\\d+)?)?        # │ optional range “–13” → “7 - 13” │\n",
    "         |                                     # ├───────────────────────────┤\n",
    "           \\(\\s*\\d+(?:\\.\\d+)?\\s*\\)             # │  parenthesized “(3)”      │\n",
    "        )                                      # └───────────────────────────┘\n",
    "        \\s*                                    # optional whitespace\n",
    "        (?:%|percent)                          # “%” or “percent”\n",
    "        \"\"\"\n",
    "    ) \n",
    "    matches = pattern.findall(commission_text)\n",
    "    \n",
    "    # convert matches from string to float\n",
    "    # if it's a range like \"7 - 13 percent\" we'll return both numbers [7.0, 13.0]\n",
    "    nums = []\n",
    "    for match in matches:\n",
    "        # strip out everything except digits, dot, and hyphen\n",
    "        cleaned = re.sub(r'[^\\d\\.\\-]', '', match)\n",
    "        if '-' in cleaned:\n",
    "            lo, hi = cleaned.split('-', 1)\n",
    "            nums.extend([float(lo), float(hi)])\n",
    "        else:\n",
    "            nums.append(float(cleaned))\n",
    "    \n",
    "    # handle the case when the % commission fee is not disclosed\n",
    "    if not nums:\n",
    "        return 0\n",
    "\n",
    "    # handle multiple % commission fee\n",
    "    # assume the presence of tiered commission when there are more than three commissions disclosed\n",
    "    if len(nums) > 3:\n",
    "        nums_sorted = sorted(nums, reverse=True)\n",
    "        group = [nums_sorted[0]] # stores tiered % commission\n",
    "        outliers = []\n",
    "        for num in nums_sorted[1:]:\n",
    "            if group[-1] - num <= 2:\n",
    "                group.append(num)\n",
    "            else:\n",
    "                outliers.append(num)\n",
    "        # compute average of the main group (i.e. tiered commission), then add outliers\n",
    "        avg_main = sum(group) / len(group)\n",
    "        final_rate = avg_main + sum(outliers)\n",
    "    # otherwise, we just average the commissions\n",
    "    else:\n",
    "        final_rate = sum(nums) / len(nums)\n",
    "        \n",
    "    \n",
    "    # handle exception, e.g. \"2.0% of total offering plus 2.85% of investment\"\n",
    "    exp_patterns = [\n",
    "        re.findall(r'(\\d+(?:\\.\\d+)?\\s*(?:%|percent)) of total offering plus (\\d+(?:\\.\\d+)?\\s*(?:%|percent))', commission_text),\n",
    "        re.findall(r'(\\d+(?:\\.\\d+)?\\s*(?:%|percent)) of total offering. (\\d+(?:\\.\\d+)?\\s*(?:%|percent)) per investment', commission_text)\n",
    "    ]\n",
    "    for exp_pattern in exp_patterns:\n",
    "        if exp_pattern:\n",
    "            # convert to float\n",
    "            first, second = float(re.sub(r'[^\\d\\.\\-]', '', exp_pattern[0][0])), float(re.sub(r'[^\\d\\.\\-]', '', exp_pattern[0][1]))\n",
    "            final_rate = first + second\n",
    "    \n",
    "    return final_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "816e2ecd-cd0c-4d96-aca1-1e865aab2dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to extract commission from the text\n",
    "ml_df['RAISEDCOMMISSION'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    ml_df.loc[idx, 'RAISEDCOMMISSION'] = extract_commission_fee(ml_df.loc[idx, 'TEXT_RAISEDCOMMISSION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c5fd1-e27b-4787-8d6b-52141887757c",
   "metadata": {},
   "source": [
    "Platform Fee (Equity Stake) Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b279fb0-3dbc-4f29-8488-a6e48492fd3d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Platform equity stake examples\n",
    "\n",
    "Platform wants equity stake\n",
    "- \"The Intermediary will also receive compensation in the form of securities equal to two percent (2%) of the total number of the securities sold in the offering.\"\n",
    "- \"The Intermediary will receive a security-compensation equal to two percent (2%) of the total number of Securities sold in the Offering.\"\n",
    "- \"Up to 2% or 200 basis points of the success fee may be taken as equity in company based on the same security pursuant to this offering.\"\n",
    "- \"Securities equal to 2.0% of the securities sold will be issued to the intermediary upon a successful fundraise.\"\n",
    "- \"SI Securities will receive equity compensation equal to 5% of the number of securities sold.\"\n",
    "\n",
    "Platform that does not want equity stake\n",
    "- \"MainVest, Inc. owns no interest in the Company, directly or indirectly, and will not acquire an interest as part of the Offering, nor is there any arrangement for MainVest, Inc. to acquire an interest.\"\n",
    "- \"The issuer will not owe a cash commission, or any other direct or indirect interest in the issuer, to the intermediary at the conclusion of the offering.\"\n",
    "- \"None\"\n",
    "- \"No\"\n",
    "- nan\n",
    "\n",
    "Multiple pecentages\n",
    "- \"the intermediary will also receive compensation in the form of securities equal to (i) 2% of the securities sold for any amounts raised up to two million dollars; (ii) 1% of the securities sold for any amounts raised b/w two million & three million dollars\"\n",
    "- \"the intermediary will also receive compensation in the form of securities equal to: (i) 2% of the securities sold for any amounts raised between $0.00- $2m; 1% for any amounts raised $2,000,000.01 - $4m; 0.5% for any amounts raised $4,000,000.01 - $5m.\"\n",
    "- \"securities commission equal to (x) two percent (2%) up to $2,000,000, plus (y) one percent (1%) above $2,000,000 up to $4,000,000, plus (z) one-half percent (0.5%) above $4,000,000.\"\n",
    "- \"a securities commission of 2% for amounts raised under $2m, 1% between $2m and $4m and .5% for amounts raised between $4m and $5m.\" \n",
    "- \"equity safe notes amounting to 2% of the dollar amount raised through the portal. all in 7% (5% commission and 2% in safe).\"\n",
    "- \"equity safe notes amounting to 2% of the dollar amount raised through the portal. all in 7% (5% commission and 2% in safe).\"\n",
    "- \"2% of raise amount in safe equity. the total fee of 7% (5% cash and 2% equity of total amount raised).\"\n",
    "- \"the intermediary will also receive compensation in the form of securities equal to (a) 2% of of any amounts raised up to $2,000,000, (b) 1% of amount raised $2,000,000.01-$4,000,000.00, (c) 0.5% of the amount raised 4,000,000.01-$5,000,000.00\"\n",
    "- \"5% commission plus 2% safe equity for a total of 7%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "0ac9ec90-74b7-4399-995e-8f3251cce932",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_equity_fee(commission_text):\n",
    "    '''\n",
    "    This function uses regex to extract % platform equity stake from CF offering. \n",
    "    '''\n",
    "    # handle cases when commission_text is np.nan\n",
    "    if pd.isna(commission_text):\n",
    "        return 0\n",
    "    \n",
    "    commission_text = commission_text.lower()\n",
    "    \n",
    "    pattern = re.compile(r'\\d+(?:\\.\\d+)?\\s*(?:%|percent)', re.IGNORECASE)\n",
    "    matches = pattern.findall(commission_text)\n",
    "    \n",
    "    # convert matches from string to float\n",
    "    nums = []\n",
    "    for match in matches:\n",
    "        # strip out everything except digits, dot, and hyphen\n",
    "        cleaned = re.sub(r'[^\\d\\.]', '', match)\n",
    "        nums.append(float(cleaned))\n",
    "    \n",
    "    # handle the case when the % equity stake fee is not disclosed\n",
    "    if not nums:\n",
    "        return 0\n",
    "\n",
    "    # handle multiple % equity stake fee\n",
    "    if len(nums) > 1:\n",
    "        if 5 in nums and 2 in nums: # There are about 10 instances of: \"5% commission plus 2% safe equity for a total of 7%\"\n",
    "            return 2\n",
    "        else:\n",
    "            return sum(nums) / len(nums)\n",
    "    # only one % equity stake fee\n",
    "    else:\n",
    "        return nums[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "997d370a-680e-42fa-b8da-b1377731e2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regex to extract equity stake fee from the text\n",
    "ml_df['EQUITYCOMMISSION'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    ml_df.loc[idx, 'EQUITYCOMMISSION'] = extract_equity_fee(ml_df.loc[idx, 'TEXT_EQUITYCOMMISSION'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a43d1ca-790b-450d-9d48-f71cd6484232",
   "metadata": {},
   "source": [
    "Platform popularity Feature\n",
    "- The mathematical formula used here is described in \"Idea Btainstorm.docx\" file\n",
    "- Even though we are using data points between 2018 and 2023 to train our ML model, in computing platform popularity, we also include data points from 2016 and 2017. This ensures that the platform popularity figure for the 2018 data points are not triveal, i.e. meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05eae5d-9526-4144-8059-c4459a6c307a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Compute platform popularity\n",
    "#     # This asusmes that the ml_df has been sorted by the FILING_NUMBER, or equivalently the FILING_DATE\n",
    "# ml_df['PLATFORMPOPULARITY'] = np.nan\n",
    "# for i in range(len(ml_df)):\n",
    "#     # Compute the starting date of the 12-month window\n",
    "#     end_window = ml_df.loc[ml_df.index[i], 'FILING_DATE']\n",
    "#     start_Window = end_window - pd.DateOffset(months=12)\n",
    "#     platform = ml_df.loc[ml_df.index[i], 'COMMISSIONCIK']\n",
    "    \n",
    "#     # Filter to only include past CFs with deadline date between FILING_DATE - 12 months and FILING_DATE\n",
    "#     total_success = 0\n",
    "#     platform_success = 0\n",
    "#     for k in range(i):\n",
    "#         past_deadline = ml_df.loc[ml_df.index[k], 'FINALDEADLINEDATE']\n",
    "#         if (start_Window <= past_deadline <= end_window) and (ml_df.loc[ml_df.index[k], 'Y'] == 1):\n",
    "#             # Update the total number of successful past CFs within the 12-month window, in all platforms\n",
    "#             total_success += 1\n",
    "            \n",
    "#             # Update the total number of successful past CFs wihtin the 12-month windoow, in the selected platform\n",
    "#             if ml_df.loc[ml_df.index[k], 'COMMISSIONCIK'] == platform:\n",
    "#                 platform_success += 1\n",
    "                \n",
    "#     # To prevent error division, i.e. 0 divided by 0\n",
    "#     if total_success > 0:\n",
    "#         ml_df.loc[ml_df.index[i], 'PLATFORMPOPULARITY'] = platform_success / total_success\n",
    "#     else:\n",
    "#         ml_df.loc[ml_df.index[i], 'PLATFORMPOPULARITY'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "5ceb6bd7-b5e0-4c4c-8080-e93884d22ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that this code does exactly the same as the above, but this one is just much faster\n",
    "dates = ml_df['FILING_DATE'].values\n",
    "deadlines = ml_df['FINALDEADLINEDATE'].values\n",
    "platforms = ml_df['COMMISSIONCIK'].values\n",
    "success = ml_df['Y'].values.astype(bool)\n",
    "\n",
    "pop_scores = []\n",
    "\n",
    "for i, (d, p) in enumerate(zip(dates, platforms)):\n",
    "    start = d - np.timedelta64(12, 'M')  # equivalent to pd.DateOffset\n",
    "    window_mask = (deadlines[:i] >= start) & (deadlines[:i] <= d) & success[:i]\n",
    "    total = window_mask.sum()\n",
    "    if total == 0:\n",
    "        pop_scores.append(0)\n",
    "    else:\n",
    "        pop_scores.append(((platforms[:i] == p) & window_mask).sum() / total)\n",
    "    \n",
    "ml_df['PLATFORMPOPULARITY'] = pop_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08fb83d-2a49-4b7f-94fa-6f330abcc47b",
   "metadata": {},
   "source": [
    "Offering Duration Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "ab38e424-cdb7-472d-9103-33cbdd34bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute offering duration\n",
    "ml_df['DURATION'] = ml_df['FINALDEADLINEDATE'] - ml_df['FILING_DATE']\n",
    "\n",
    "# Change data type from datetime to float\n",
    "ml_df['DURATION'] = ml_df['DURATION'].dt.days.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c276cf-ba08-4ea4-a674-f2ea9752c3a1",
   "metadata": {},
   "source": [
    "Number of Competitors Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a1af55-4887-43e1-9510-5e38fc074997",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # To store the number of CF competitors\n",
    "# ml_df['COMPETITORSCOUNT'] = np.nan\n",
    "\n",
    "# # Compute the number of CF competitors\n",
    "# # Note that that ml_df dataset has been sorted by FILE_NUMBER (which also means that it has been sorted by FILING_DATE)\n",
    "# for i in range(len(ml_df)):\n",
    "#     curr_start_date = ml_df.iloc[i,:]['FILING_DATE']\n",
    "#     count_competitions = 0\n",
    "    \n",
    "#     # count the number of comptitors with FILING_DATE before the currently selected CF\n",
    "#     for k in range(i):\n",
    "#         comp_deadline_date = ml_df.iloc[k,:]['FINALDEADLINEDATE']\n",
    "#         if comp_deadline_date >= curr_start_date:\n",
    "#             count_competitions += 1\n",
    "    \n",
    "#     # count the number of competitors with the same FILING_DATE as the currently selected CF\n",
    "#     add = 1\n",
    "#     while i + add < len(ml_df) and ml_df.iloc[i + add,:]['FILING_DATE'] == curr_start_date:\n",
    "#         count_competitions += 1\n",
    "#         add += 1\n",
    "    \n",
    "#     ml_df.loc[ml_df.index[i], 'COMPETITORSCOUNT'] = count_competitions\n",
    "    \n",
    "#     if i == 100:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "b407b8e6-9c3f-4784-8c02-4a68f25048fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: This cose does the exact thing as the above, the only difference is the time complexity, which has been improved\n",
    "# ... from O(n^2) to O(n log(n))\n",
    "import heapq\n",
    "\n",
    "# assume ml_df is already sorted by 'FILING_DATE'\n",
    "ml_df_temp = ml_df.copy()\n",
    "\n",
    "# 1) prior_active_count via min‐heap of deadlines\n",
    "deadlines = []          # min‐heap of previous deadlines\n",
    "prior_active = []       # to store counts\n",
    "\n",
    "for start, deadline in zip(ml_df_temp['FILING_DATE'], ml_df_temp['FINALDEADLINEDATE']):\n",
    "    # pop any deadlines that have already \"closed\" before this start\n",
    "    while deadlines and deadlines[0] < start:\n",
    "        heapq.heappop(deadlines)\n",
    "    # now len(deadlines) is the number of previous intervals still open\n",
    "    prior_active.append(len(deadlines))\n",
    "    # push this offering's deadline for future starts\n",
    "    heapq.heappush(deadlines, deadline)\n",
    "\n",
    "ml_df_temp['PRIOR_ACTIVE'] = prior_active\n",
    "\n",
    "# 2) count how many same‐date peers come *after* each row\n",
    "#    group_size = total per date, cum_idx = 0,1,2… in group\n",
    "group_size = ml_df_temp.groupby('FILING_DATE')['FILING_DATE'].transform('size')\n",
    "cum_idx    = ml_df_temp.groupby('FILING_DATE').cumcount()\n",
    "peers_after = group_size - cum_idx - 1\n",
    "\n",
    "ml_df_temp['PEERS_AFTER'] = peers_after\n",
    "\n",
    "# 3) competitor count = prior_active + peers_after\n",
    "ml_df_temp['COMPETITORSCOUNT'] = ml_df_temp['PRIOR_ACTIVE'] +ml_df_temp['PEERS_AFTER']\n",
    "\n",
    "# cleanup\n",
    "ml_df_temp = ml_df_temp.drop(columns=['PRIOR_ACTIVE','PEERS_AFTER'])\n",
    "\n",
    "ml_df['COMPETITORSCOUNT'] = ml_df_temp['COMPETITORSCOUNT']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71a319c-7323-40ed-b231-018a3ab91dfa",
   "metadata": {},
   "source": [
    "Type of Security Offered Feature: Equity, Debt, SAFE, or Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "c6db98ae-3d07-49e4-9696-6d4cdd12fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the categories\n",
    "ml_df['SECURITYOFFEREDTYPE'] = ml_df['SECURITYOFFEREDTYPE'].map({\n",
    "    'Other': 'Complex',\n",
    "    'Common Stock': 'Equity',\n",
    "    'Debt': 'Debt',\n",
    "    'Preferred Stock': 'Equity'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "2454f22e-3e85-49a1-9344-a5c6a6a44cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_security_offered(text):\n",
    "    '''\n",
    "    This function uses regex to classify the type of secutiy offered in a CF offering.\n",
    "    The function checks for patterns in a specific order: SAFE > Debt > Equity > Other.\n",
    "    '''\n",
    "    # Special case: When text is np.nan it means that the security type has been disclosed in the SECURITYOFFEREDTYPE column\n",
    "    if pd.isna(text):\n",
    "        return np.nan\n",
    "    \n",
    "    txt = text.lower()\n",
    "    \n",
    "    # 1) SAFE\n",
    "    # matches 'simple agreement(s) for future equity', 'safe', 'safe+rev'\n",
    "    if re.search(r'simple agreement(?:s)? for future equity', txt) or re.search(r'\\bsafe\\b', txt):\n",
    "        return 'SAFE'\n",
    "    \n",
    "    # 2) Debt\n",
    "    # matches DPA, DPAs, or DUSTO\n",
    "    if re.search(r'\\bdpa(?:s)?\\b', txt) or re.search(r'\\bdusto\\b', txt):\n",
    "        return 'Debt'\n",
    "    \n",
    "    # 3) Equity\n",
    "    # check multi-word patterns first\n",
    "    equity_patterns = [\n",
    "        r'\\bcommon unit(?:s)?\\b',\n",
    "        r'\\bllc unit(?:s)?\\b',\n",
    "        r'\\bmember unit(?:s)?\\b',\n",
    "        r'\\bmembership interest(?:s)?\\b',\n",
    "        r'\\bpartnership equity\\b',\n",
    "        r'\\bpreferred\\b',\n",
    "        r'\\bclass\\b',\n",
    "        r'\\bseries\\b',\n",
    "        r'\\bcommon\\b'\n",
    "    ]\n",
    "    for pat in equity_patterns:\n",
    "        if re.search(pat, txt):\n",
    "            return 'Equity'\n",
    "    \n",
    "    # 4) Others\n",
    "    return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "2ec19fb7-e74e-4049-a585-60bb23c4b995",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract security type from SECURITYOFFEREDOTHERDESC\n",
    "ml_df['EXTRACTED_SECURITYOFFEREDOTHERDESC'] = np.nan\n",
    "for i in range(len(ml_df)):\n",
    "    text = ml_df.iloc[i,:]['SECURITYOFFEREDOTHERDESC']\n",
    "    ml_df.loc[ml_df.index[i], 'EXTRACTED_SECURITYOFFEREDOTHERDESC'] = extract_security_offered(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "82396714-b2cf-4045-9620-848439bbc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine the results from SECURITYOFFEREDTYPE and EXTRACTED_SECURITYOFFEREDOTHERDESC\n",
    "ml_df['FINALSECURITYOFFEREDTYPE'] = np.nan\n",
    "for i in range(len(ml_df)):\n",
    "    if ml_df.iloc[i,:]['SECURITYOFFEREDTYPE'] == 'Complex':\n",
    "        ml_df.loc[ml_df.index[i], 'FINALSECURITYOFFEREDTYPE'] = ml_df.iloc[i,:]['EXTRACTED_SECURITYOFFEREDOTHERDESC']\n",
    "    else:\n",
    "        ml_df.loc[ml_df.index[i], 'FINALSECURITYOFFEREDTYPE'] = ml_df.iloc[i,:]['SECURITYOFFEREDTYPE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfea1ff-38c8-4a8a-9215-df77fab3fcfb",
   "metadata": {},
   "source": [
    "Price Determination Method Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "cb14c0e6-501d-4f20-bc36-d36e5261ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pricing_method(text, sec_type):\n",
    "    \"\"\"\n",
    "    Return 0 if pricing is ARBITRARY, 1 if NON-ARBITRARY.\n",
    "    \n",
    "    text: str or NaN\n",
    "    sec_type: 'SAFE', 'Equity', etc.\n",
    "    \"\"\"\n",
    "    # 0) methodology not specified\n",
    "    if pd.isna(text):\n",
    "        return 0\n",
    "    \n",
    "    # 1) normalize\n",
    "    t = text.lower().strip()\n",
    "    \n",
    "    # 2) obvious arbitrary keywords\n",
    "    arb_kw = [\n",
    "        r'\\barbitrar(?:y|ity)\\b',\n",
    "        r'\\bdiscretion\\b',\n",
    "        r'to be determined'\n",
    "    ]\n",
    "    for p in arb_kw:\n",
    "        if re.search(p, t):\n",
    "            return 0\n",
    "    \n",
    "    # 3) SAFE‐specific arbitrary patterns\n",
    "    if sec_type.upper() == 'SAFE':\n",
    "        safe_patterns = [\n",
    "            r'price will be set upon conversion after a qualifying event in the future',\n",
    "            r'^\\d+(?:,\\d{3})*(?:\\.\\d+)?$',                      # only a number like \"1.0\", no other text description\n",
    "            r'\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?\\s*per '\n",
    "        ]\n",
    "        for p in safe_patterns:\n",
    "            if re.search(p, t):\n",
    "                return 0\n",
    "    \n",
    "    # 4) Equity‐specific arbitrary patterns\n",
    "    if sec_type.lower() == 'equity':\n",
    "        eq_patterns = [\n",
    "            r'\\bmanagement\\b',                              # management determination\n",
    "            r'\\bmanaging\\b',\n",
    "            r'\\$\\s*\\d+(?:,\\d{3})*(?:\\.\\d+)?\\s*per ',        # $1.00 per share / unit\n",
    "            r'based on target number of securities',        # formulaic but non-traditional\n",
    "            r'good faith'                                   # good faith determination\n",
    "        ]\n",
    "        for p in eq_patterns:\n",
    "            if re.search(p, t):\n",
    "                return 0\n",
    "    \n",
    "    # 5) If none of the above matched, assume NON-ARBITRARY\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "10679476-ce86-4285-9ad3-f77ac85f61ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract on whether pricing methodology was arbitrary or not, using regex\n",
    "ml_df['IS_PRICEMETHOD'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    ml_df.loc[idx, 'IS_PRICEMETHOD'] = extract_pricing_method(ml_df.loc[idx, 'PRICEDETERMINATIONMETHOD'], ml_df.loc[idx, 'FINALSECURITYOFFEREDTYPE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec58bfa-28ef-4b3b-a8f3-f3987fb513ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Issuer Characteristics Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4eacbd5-6e54-4a3a-a5fb-e6dc22fcbed7",
   "metadata": {},
   "source": [
    "Company Age Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1d6f1d51-c970-43ea-9fd0-94b6dc87e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compaute the company age\n",
    "ml_df['COMPANYAGE'] = (ml_df['FILING_DATE'] - ml_df['DATEINCORPORATION']).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcaac9b-d7a0-4f8b-8d87-063ac4c9bf94",
   "metadata": {},
   "source": [
    "Number of Past Successful CF (By The Same Issuer) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "a31d9435-dca8-4da1-85bd-afacae4a90ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of past successful CF, by the same issuer\n",
    "# Step 1: Sort the entire DataFrame first by 'CIK' and then by 'FILING_DATE'\n",
    "ml_df_sorted = ml_df.sort_values(by=['CIK', 'FILING_DATE'], ascending=[True, True])\n",
    "\n",
    "# Step 2: Group by 'CIK' and compute the cumulative sum of 'Y'\n",
    "ml_df_sorted['CUMULATIVE_Y'] = ml_df_sorted.groupby('CIK')['Y'].cumsum()\n",
    "\n",
    "# Step 3: Subtract the current row’s Y to leave only *prior* successes\n",
    "ml_df_sorted['PASTCOMPANYSUCCESS'] = ml_df_sorted['CUMULATIVE_Y'] - ml_df_sorted['Y']\n",
    "\n",
    "# Step 4: Transfer the PASTCOMPANYSUCCESS from ml_df_sorted to ml_df\n",
    "ml_df['PASTCOMPANYSUCCESS'] = np.nan\n",
    "for i in range(len(ml_df_sorted)):\n",
    "    ml_df.loc[ml_df_sorted.index[i], 'PASTCOMPANYSUCCESS'] = ml_df_sorted.iloc[i,:]['PASTCOMPANYSUCCESS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e280da-6524-4907-9b27-6a9dba5f0fc8",
   "metadata": {},
   "source": [
    "Number of Past Failed CF (By The Same Issuer) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "39fae75b-aedf-40ed-86d2-d140ddb97e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of past failed CF, by the same issuer\n",
    "# Step 1.1: Sort the entire DataFrame first by 'CIK' and then by 'FILING_DATE'\n",
    "ml_df_sorted = ml_df.sort_values(by=['CIK', 'FILING_DATE'], ascending=[True, True])\n",
    "\n",
    "# Step 1.2: Compute the reverse of Y, i.e. 0 becomes 1 and 1 becomes 0\n",
    "ml_df_sorted['REVERSE_Y'] = ml_df_sorted['Y'].map({0:1, 1:0})\n",
    "\n",
    "# Step 2: Group by 'CIK' and compute the cumulative sum of 'REVERSE_Y'\n",
    "ml_df_sorted['CUMULATIVE_REVERSE_Y'] = ml_df_sorted.groupby('CIK')['REVERSE_Y'].cumsum()\n",
    "\n",
    "# Step 3: Subtract the current row’s REVERSE_Y to leave only *prior* failures\n",
    "ml_df_sorted['PASTCOMPANYFAILURE'] = ml_df_sorted['CUMULATIVE_REVERSE_Y'] - ml_df_sorted['REVERSE_Y']\n",
    "\n",
    "# Step 4: Transfer the PASTCOMPANYFAILURE from ml_df_sorted to ml_df\n",
    "ml_df['PASTCOMPANYFAILURE'] = np.nan\n",
    "for i in range(len(ml_df_sorted)):\n",
    "    ml_df.loc[ml_df_sorted.index[i], 'PASTCOMPANYFAILURE'] = ml_df_sorted.iloc[i,:]['PASTCOMPANYFAILURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75356215-99e0-4d03-b967-a0bc0536e243",
   "metadata": {},
   "source": [
    "Number of Past Successful CF (By The Issuing Person) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "50a74874-9df4-46ad-bffd-c009ea5acb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the issuing person's name\n",
    "    # We need to take car of the following.\n",
    "        # There is this weired tag, e.g. \"/s/ Adam Berry\" and \"/c/ Charles Potter\"\n",
    "        # There are also cases where, e.g. \"/Gregory Miller/\"\n",
    "        # There are also cases where there are multiple white-spces, e.g. \"Bruce  Goldblatt\"\n",
    "        # Remove title, e.g. 'John Terry, PhD', 'Johnathan Aho, M.D., PhD', 'Dr.' etc.\n",
    "\n",
    "ml_df['ISSUERSIGNATURE'] = (\n",
    "    ml_df['ISSUERSIGNATURE']\n",
    "      # 0) lowercase everything\n",
    "      .str.lower()\n",
    "      # 1) remove /s/, /c/, any single-letter tags and stray slashes\n",
    "      .str.replace(r'/[a-z]/\\s*|/', '', regex=True)\n",
    "      # 2) strip leading \"dr.\"\n",
    "      .str.replace(r'^\\s*dr\\.?\\s+', '', regex=True)\n",
    "      # 3) strip trailing \", phd\", \", m.d.\", \", m.d., phd\", etc.\n",
    "      .str.replace(\n",
    "          r',\\s*(?:m\\.?d\\.?|ph\\.?d\\.?)(?:\\s*,\\s*(?:m\\.?d\\.?|ph\\.?d\\.?))*\\s*$',\n",
    "          '',\n",
    "          regex=True\n",
    "      )\n",
    "      # 4) collapse multiple white-spaces into one\n",
    "      .str.replace(r'\\s+', ' ', regex=True)\n",
    "      # 5) remove trailing and the opposite of trailing white-spaces\n",
    "      .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "4f2de205-5b91-47c8-9491-dd4cac95fd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of past successful CF, by the same issuing person\n",
    "# Step 1: Sort the entire DataFrame first by 'ISSUERSIGNATURE' and then by 'FILING_DATE'\n",
    "ml_df_sorted = ml_df.sort_values(by=['ISSUERSIGNATURE', 'FILING_DATE'], ascending=[True, True])\n",
    "\n",
    "# Step 2: Group by 'CIK' and compute the cumulative sum of 'Y'\n",
    "ml_df_sorted['CUMULATIVE_Y'] = ml_df_sorted.groupby('ISSUERSIGNATURE')['Y'].cumsum()\n",
    "\n",
    "# Step 3: Subtract the current row’s Y to leave only *prior* successes\n",
    "ml_df_sorted['PASTPERSONSUCCESS'] = ml_df_sorted['CUMULATIVE_Y'] - ml_df_sorted['Y']\n",
    "\n",
    "# Step 4: Transfer the PASTPERSONSUCCESS from ml_df_sorted to ml_df\n",
    "ml_df['PASTPERSONSUCCESS'] = np.nan\n",
    "for i in range(len(ml_df_sorted)):\n",
    "    ml_df.loc[ml_df_sorted.index[i], 'PASTPERSONSUCCESS'] = ml_df_sorted.iloc[i,:]['PASTPERSONSUCCESS']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a05e6-f561-4eac-ab12-1f9d358b2c82",
   "metadata": {},
   "source": [
    "Number of Past Failed CF (By The Issuing Person) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "ee475ad1-e317-42c1-87a6-a81f196544d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of past failed CF, by the same issuer\n",
    "# Step 1.1: Sort the entire DataFrame first by 'ISSUERSIGNATURE' and then by 'FILING_DATE'\n",
    "ml_df_sorted = ml_df.sort_values(by=['ISSUERSIGNATURE', 'FILING_DATE'], ascending=[True, True])\n",
    "\n",
    "# Step 1.2: Compute the reverse of Y, i.e. 0 becomes 1 and 1 becomes 0\n",
    "ml_df_sorted['REVERSE_Y'] = ml_df_sorted['Y'].map({0:1, 1:0})\n",
    "\n",
    "# Step 2: Group by 'ISSUERSIGNATURE' and compute the cumulative sum of 'REVERSE_Y'\n",
    "ml_df_sorted['CUMULATIVE_REVERSE_Y'] = ml_df_sorted.groupby('ISSUERSIGNATURE')['REVERSE_Y'].cumsum()\n",
    "\n",
    "# Step 3: Subtract the current row’s REVERSE_Y to leave only *prior* failures\n",
    "ml_df_sorted['PASTPERSONFAILURE'] = ml_df_sorted['CUMULATIVE_REVERSE_Y'] - ml_df_sorted['REVERSE_Y']\n",
    "\n",
    "# Step 4: Transfer the PASTCOMPANYFAILURE from ml_df_sorted to ml_df\n",
    "ml_df['PASTPERSONFAILURE'] = np.nan\n",
    "for i in range(len(ml_df_sorted)):\n",
    "    ml_df.loc[ml_df_sorted.index[i], 'PASTPERSONFAILURE'] = ml_df_sorted.iloc[i,:]['PASTPERSONFAILURE']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307af838-3454-4ce4-a039-05c127eb1bda",
   "metadata": {},
   "source": [
    "Incorporated in Delaware Boolean Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "a64ee673-4032-482f-8900-b58283a23b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain whether the company was registered in Delaware (DE) or not\n",
    "ml_df['ISDELAWARE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    if ml_df.loc[idx, 'JURISDICTIONORGANIZATION'] == 'DE':\n",
    "        ml_df.loc[idx, 'ISDELAWARE'] = 1\n",
    "    else:\n",
    "        ml_df.loc[idx, 'ISDELAWARE'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c16b03-f796-46d7-a8a3-babf5d232a27",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Geographic Characteristics Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7dc073-b719-488c-88df-f0b2566452f6",
   "metadata": {},
   "source": [
    "Local Competition Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "be002515-4911-42a2-b521-d47ea1507ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply filter to only include companies that are operating in the USA\n",
    "us_states = {\n",
    "    'AL','AK','AZ','AR','CA','CO','CT','DE','FL','GA','HI','IA','ID','IL','IN',\n",
    "    'KS','KY','LA','MA','MD','ME','MI','MN','MO','MS','MT','NC','ND','NE','NH',\n",
    "    'NJ','NM','NV','NY','OH','OK','OR','PA','RI','SC','SD','TN','TX','UT','VA',\n",
    "    'VT','WA','WI','WV','WY'\n",
    "}\n",
    "\n",
    "mask = ml_df['STATEORCOUNTRY'].isin(us_states)\n",
    "ml_df = ml_df.loc[mask, :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f6ea46-480e-4c81-a9c2-1e14e756deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To store the number of local CF competitors\n",
    "# ml_df['LOCALCOMPETITORSCOUNT'] = np.nan\n",
    "\n",
    "# # Compute the number of local CF competitors\n",
    "# # Note that that ml_df dataset has been sorted by FILE_NUMBER (which also means that it has been sorted by FILING_DATE)\n",
    "# for i in range(len(ml_df)):\n",
    "#     curr_start_date = ml_df.iloc[i,:]['FILING_DATE']\n",
    "#     curr_state = ml_df.iloc[i,:]['STATEORCOUNTRY']\n",
    "#     count_competitions = 0\n",
    "    \n",
    "#     # count the number of comptitors in the same state with FILING_DATE before the currently selected CF\n",
    "#     for k in range(i):\n",
    "#         comp_deadline_date = ml_df.iloc[k,:]['FINALDEADLINEDATE']\n",
    "#         comp_state = ml_df.iloc[k,:]['STATEORCOUNTRY']\n",
    "#         if comp_deadline_date >= curr_start_date and comp_state == curr_state:\n",
    "#             count_competitions += 1\n",
    "    \n",
    "#     # count the number of competitors with the same FILING_DATE as the currently selected CF\n",
    "#     add = 1\n",
    "#     while i + add < len(ml_df) and ml_df.iloc[i + add,:]['FILING_DATE'] == curr_start_date and ml_df.iloc[i + add,:]['STATEORCOUNTRY'] == curr_state:\n",
    "#         count_competitions += 1\n",
    "#         add += 1\n",
    "    \n",
    "#     ml_df.loc[ml_df.index[i], 'LOCALCOMPETITORSCOUNT'] = count_competitions\n",
    "    \n",
    "#     if i == 200:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "bae715d7-18a9-451e-b920-20feb3297185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the exact same as the above, but it is a lot faster\n",
    "\n",
    "# Prepare a heap per state\n",
    "deadlines_by_state = {state: [] for state in ml_df['STATEORCOUNTRY'].unique()}\n",
    "\n",
    "# Compute local competitor counts\n",
    "local_counts = []\n",
    "for start, deadline, state in zip(ml_df['FILING_DATE'],\n",
    "                                 ml_df['FINALDEADLINEDATE'],\n",
    "                                 ml_df['STATEORCOUNTRY']):\n",
    "    heap = deadlines_by_state[state]\n",
    "    # Remove any deadlines that ended before this start date\n",
    "    while heap and heap[0] < start:\n",
    "        heapq.heappop(heap)\n",
    "    # Now heap contains only past CFs in this state still “active”\n",
    "    local_counts.append(len(heap))\n",
    "    # Push this CF’s deadline for future rows\n",
    "    heapq.heappush(heap, deadline)\n",
    "\n",
    "ml_df['LOCALCOMPETITORSCOUNT'] = local_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d4e44f-d92d-43f4-bacb-ffaf4f63b489",
   "metadata": {},
   "source": [
    "Creative Workforce Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "6146367a-5990-463d-bf38-4f8f9879ff12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Process workforce dataset\n",
    "creative_df = pd.DataFrame({'YEAR':[], 'STATE':[], 'PROPORTION1000':[]})\n",
    "for i in range(16,25):\n",
    "    folder_name = f'oesm{i}st'\n",
    "    file_name = f'state_M20{i}_dl.xlsx'\n",
    "\n",
    "    # import the excel file\n",
    "    file_path = f'demographic_dataset/workforce_dataset/{folder_name}/{file_name}'\n",
    "    workforce_df = pd.read_excel(file_path)\n",
    "    \n",
    "    # extract the proportion per 1000 of people working in creative industry in each state\n",
    "    if 'ST' in workforce_df.columns: # Between 2016 and 2019, state is stored in 'ST' column\n",
    "        for state in workforce_df['ST'].unique():\n",
    "            mask = (workforce_df['ST'] == state) & (workforce_df['OCC_TITLE'] == 'Arts, Design, Entertainment, Sports, and Media Occupations')\n",
    "            creative_proportion = workforce_df.loc[mask, 'JOBS_1000'].iloc[0]\n",
    "            creative_df.loc[len(creative_df)] = [i + 2000, state, creative_proportion]\n",
    "    elif 'PRIM_STATE' in workforce_df.columns: # Between 2020 and 2025, state is stored in 'PRIM_STATE' column\n",
    "        for state in workforce_df['PRIM_STATE'].unique():\n",
    "            mask = (workforce_df['PRIM_STATE'] == state) & (workforce_df['OCC_TITLE'] == 'Arts, Design, Entertainment, Sports, and Media Occupations')\n",
    "            creative_proportion = workforce_df.loc[mask, 'JOBS_1000'].iloc[0]\n",
    "            creative_df.loc[len(creative_df)] = [i + 2000, state, creative_proportion]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "be43c3d1-f24c-4073-a7fe-423e78201eb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add the proportion of creative workforce to ml_df\n",
    "ml_df['CREATIVEWORKFORCE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = ml_df.loc[idx, 'FILING_DATE'].year\n",
    "    mask = (creative_df['STATE'] == curr_state) & (creative_df['YEAR'] == curr_year)\n",
    "    ml_df.loc[idx, 'CREATIVEWORKFORCE'] = creative_df.loc[mask, 'PROPORTION1000'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee405af8-4739-4954-a6c6-8a4dd2e0885c",
   "metadata": {},
   "source": [
    "RUCC (Rural-Urban Continuum Code) Feature\n",
    "- This measure the centrality / peripherility of US states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "4c3a5891-063d-4faa-8b54-99c543c7c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import RUCC data\n",
    "file_path = 'demographic_dataset/rucc_dataset/Ruralurbancontinuumcodes2023.xlsx'\n",
    "rucc_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "40c7a3fd-3a73-4ee3-bf86-72d40727f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Compute total population and population-weighted RUCC sum per state\n",
    "pop_sum = rucc_df.groupby('State')['Population_2020'].sum()\n",
    "rucc_pop = (rucc_df['RUCC'] * rucc_df['Population_2020']).groupby(rucc_df['State']).sum()\n",
    "\n",
    "# 2) Weighted average RUCC per state\n",
    "state_rucc = (rucc_pop / pop_sum).rename('RUCC')\n",
    "\n",
    "# 3) Map into ml_df\n",
    "ml_df['RUCC'] = ml_df['STATEORCOUNTRY'].map(state_rucc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c689fd8f-b78c-4b3f-821a-83fbf767a159",
   "metadata": {},
   "source": [
    "Income Level Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "48d84bc0-9d6b-4a5a-8194-429a4aa64986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import personal income per capita\n",
    "file_path = 'demographic_dataset/income_dataset/income_dataset.xlsx'\n",
    "income_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "cf0be394-fd4a-4f0e-a514-a8218e98d2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the income level to ml_df\n",
    "ml_df['INCOMELEVEL'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = ml_df.loc[idx, 'FILING_DATE'].year\n",
    "    mask = (income_df['STATE_CODE'] == curr_state)\n",
    "    ml_df.loc[idx, 'INCOMELEVEL'] = income_df.loc[mask, curr_year].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef44b8a-77aa-4a12-ad78-bd12f309bbf5",
   "metadata": {},
   "source": [
    "Age Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "126323fd-dda6-44da-a5c8-4606b4c60f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import median age dataset\n",
    "file_path = 'demographic_dataset/age_dataset/ACSST1Y2023.xlsx'\n",
    "age_df = pd.read_excel(file_path)\n",
    "\n",
    "# Process the age_df\n",
    "age_df = age_df.iloc[:, [i for i in range(1, len(age_df.columns), 12)]]\n",
    "new_cols = []\n",
    "for col in age_df.columns:\n",
    "    col = col.lower()\n",
    "    match = re.findall(r'([a-z\\s]+).*', col)[0]\n",
    "    new_cols.append(match)\n",
    "age_df.columns = new_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "e57ac43e-9e70-47e9-9557-ca7d5555f477",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 2) map of lower‐case state names → postal codes\n",
    "state_name_to_code = {\n",
    "    'alabama':              'AL',\n",
    "    'alaska':               'AK',\n",
    "    'arizona':              'AZ',\n",
    "    'arkansas':             'AR',\n",
    "    'california':           'CA',\n",
    "    'colorado':             'CO',\n",
    "    'connecticut':          'CT',\n",
    "    'delaware':             'DE',\n",
    "    'district of columbia': 'DC',\n",
    "    'florida':              'FL',\n",
    "    'georgia':              'GA',\n",
    "    'hawaii':               'HI',\n",
    "    'idaho':                'ID',\n",
    "    'illinois':             'IL',\n",
    "    'indiana':              'IN',\n",
    "    'iowa':                 'IA',\n",
    "    'kansas':               'KS',\n",
    "    'kentucky':             'KY',\n",
    "    'louisiana':            'LA',\n",
    "    'maine':                'ME',\n",
    "    'maryland':             'MD',\n",
    "    'massachusetts':        'MA',\n",
    "    'michigan':             'MI',\n",
    "    'minnesota':            'MN',\n",
    "    'mississippi':          'MS',\n",
    "    'missouri':             'MO',\n",
    "    'montana':              'MT',\n",
    "    'nebraska':             'NE',\n",
    "    'nevada':               'NV',\n",
    "    'new hampshire':        'NH',\n",
    "    'new jersey':           'NJ',\n",
    "    'new mexico':           'NM',\n",
    "    'new york':             'NY',\n",
    "    'north carolina':       'NC',\n",
    "    'north dakota':         'ND',\n",
    "    'ohio':                 'OH',\n",
    "    'oklahoma':             'OK',\n",
    "    'oregon':               'OR',\n",
    "    'pennsylvania':         'PA',\n",
    "    'rhode island':         'RI',\n",
    "    'south carolina':       'SC',\n",
    "    'south dakota':         'SD',\n",
    "    'tennessee':            'TN',\n",
    "    'texas':                'TX',\n",
    "    'utah':                 'UT',\n",
    "    'vermont':              'VT',\n",
    "    'virginia':             'VA',\n",
    "    'washington':           'WA',\n",
    "    'west virginia':        'WV',\n",
    "    'wisconsin':            'WI',\n",
    "    'wyoming':              'WY',\n",
    "    'puerto rico':          'PR'\n",
    "}\n",
    "\n",
    "# 3) rename in place\n",
    "age_df.rename(columns=state_name_to_code, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "c3a03c2d-be89-4ec2-b2c2-4297af187605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the age median to ml_df\n",
    "ml_df['AGE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    ml_df.loc[idx, 'AGE'] = age_df.loc[0, curr_state]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79054a1-fbab-41b6-9b49-134607e54fb8",
   "metadata": {},
   "source": [
    "Education Feature\n",
    "- The proportion of people aged 25 or older with at least a Bachelor's degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "ed9f08c3-5c48-4cc5-ac2d-9b09a18c4945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import education dataset\n",
    "file_path = 'demographic_dataset/education_dataset/education_dataset.xlsx'\n",
    "education_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "ca0926a0-914b-4523-b1b7-7df077b8eda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the education feature to ml_df\n",
    "ml_df['BACHELOREDUCATION'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = ml_df.loc[idx, 'FILING_DATE'].year\n",
    "    mask = (education_df['YEAR'] == curr_year)\n",
    "    ml_df.loc[idx, 'BACHELOREDUCATION'] = education_df.loc[mask, curr_state].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "id": "566277b9-8fe4-4878-8392-021e52026f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_geochar = ml_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deab75a2-af5d-4137-b915-d9724b9962ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Macro-Economy Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848115e1-742f-4c14-9ac1-83093d69c469",
   "metadata": {},
   "source": [
    "Unemployment Rate Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "52be9e0b-a0ad-4050-98b8-111387ce4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import unemployment rate\n",
    "file_path = 'macroeconomy_dataset/unemployment_dataset/unemployment_dataset.xlsx'\n",
    "unemployment_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "7b1d1d71-0a8e-4ddf-a97b-40b695baf66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the unemployment rate feature to ml_df\n",
    "ml_df['UNEMPLOYMENTRATE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = ml_df.loc[idx, 'FILING_DATE'].year\n",
    "    mask = (unemployment_df['Year'] == curr_year)\n",
    "    ml_df.loc[idx, 'UNEMPLOYMENTRATE'] = unemployment_df.loc[mask, curr_state].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921dfe02-be0e-4040-ac82-ae0da8c4b1c4",
   "metadata": {},
   "source": [
    "Spot Interest Rate Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "7cdd07c0-4da9-46e3-a6b0-2bd98703c363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 10-year US governmnet bond yield\n",
    "file_path = 'macroeconomy_dataset/interestrate_dataset/interestrate_dataset.xlsx'\n",
    "interestrate_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "3aea51a3-8196-4285-a06a-80d0660d2c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use forward-fill to fill up interest rate during the weekend\n",
    "interestrate_df.set_index('observation_date', inplace=True)\n",
    "full_date = pd.date_range(start=interestrate_df.index.min(),\n",
    "                         end=interestrate_df.index.max(),\n",
    "                         freq='D')\n",
    "interestrate_df = interestrate_df.reindex(full_date).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "9e0c17c2-c81b-439e-8ae4-f2e3f059ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the spot interest rate feature to ml_df\n",
    "ml_df['SPOTINTERESTRATE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_date = ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)\n",
    "    ml_df.loc[idx, 'SPOTINTERESTRATE'] = interestrate_df.loc[curr_date, 'DGS10']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8a0241-1d3c-4146-8bbd-18f4b762f383",
   "metadata": {},
   "source": [
    "Short-Term Average Interest Rate Feature\n",
    "- Compute 30-day average, up to the start offering date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "f8e9c19a-404d-4d3f-a87d-eef4d1763ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import 10-year US governmnet bond yield\n",
    "file_path = 'macroeconomy_dataset/interestrate_dataset/interestrate_dataset.xlsx'\n",
    "interestrate_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "1495a48d-942c-4472-b5d0-179b2d15fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 30-day average\n",
    "interestrate_df['AVG'] = interestrate_df['DGS10'].rolling(window=30, min_periods=1).mean()\n",
    "\n",
    "# use forward-fill to fill up average interest rate during the weekend\n",
    "interestrate_df.set_index('observation_date', inplace=True)\n",
    "full_date = pd.date_range(start=interestrate_df.index.min(),\n",
    "                         end=interestrate_df.index.max(),\n",
    "                         freq='D')\n",
    "interestrate_df = interestrate_df.reindex(full_date).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "3132b6f3-f841-4dc7-a484-e446c7f7e900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the average interest rate feature to ml_df\n",
    "ml_df['AVERAGEINTERESTRATE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_date = ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)\n",
    "    ml_df.loc[idx, 'AVERAGEINTERESTRATE'] = interestrate_df.loc[curr_date, 'AVG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23bd90f-6292-4308-bfc4-b213a545517b",
   "metadata": {},
   "source": [
    "Inflation Rate Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "4c8bd27f-8652-42dc-b7a2-2238732cb39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 12-month change in CPI index to approximate inflation rate\n",
    "combined_cpi_df = pd.DataFrame({'Region':[], 'Year':[], 'Month':[], 'INFLATIONRATE':[]})\n",
    "census_regions = ['midwest', 'northeast', 'south', 'west']\n",
    "for region in census_regions:\n",
    "    # Import the cpi index\n",
    "    file_path = f'macroeconomy_dataset/cpi_dataset/cpi_{region}.xlsx'\n",
    "    cpi_df = pd.read_excel(file_path)\n",
    "    cpi_df.set_index('Year', inplace=True)\n",
    "    cpi_df.drop(['Annual', 'HALF1', 'HALF2'], axis=1, inplace=True)\n",
    "    \n",
    "    # Unpivot the dataset\n",
    "    cpi_df = cpi_df.stack() # unpivot the dataset\n",
    "    cpi_df = cpi_df.reset_index().rename(columns={'level_1':'Month', 0:'CPI'})\n",
    "    \n",
    "    # Compute the 12-month change in CPI index to approximate inflation rate\n",
    "    cpi_df['INFLATIONRATE'] = cpi_df['CPI'].pct_change(periods=12) * 100\n",
    "    \n",
    "    # Combine the dataset\n",
    "    cpi_df['Region'] = region\n",
    "    combined_cpi_df = pd.concat(\n",
    "        [combined_cpi_df, cpi_df[['Region','Year','Month','INFLATIONRATE']]],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "# Rename the month, e.g. Jan -> 1, Feb -> 2\n",
    "month_map = {\n",
    "    'Jan': 1, 'Feb': 2, 'Mar': 3, 'Apr': 4,\n",
    "    'May': 5, 'Jun': 6, 'Jul': 7, 'Aug': 8,\n",
    "    'Sep': 9, 'Oct': 10,'Nov': 11,'Dec': 12\n",
    "}\n",
    "combined_cpi_df['Month'] = combined_cpi_df['Month'].map(month_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "id": "a9e3bff2-e090-4c04-8746-2df959cbeb75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the inflation rate feature to ml_df\n",
    "ml_df['INFLATIONRATE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_month = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).month\n",
    "    \n",
    "    # create mapping for State -> Census Region\n",
    "    state_to_region = {\n",
    "        # Northeast\n",
    "        **dict.fromkeys(['CT','ME','MA','NH','RI','VT','NJ','NY','PA'], 'northeast'),\n",
    "        # Midwest\n",
    "        **dict.fromkeys(['IL','IN','MI','OH','WI','IA','KS','MN','MO','NE','ND','SD'], 'midwest'),\n",
    "        # South\n",
    "        **dict.fromkeys([\n",
    "            'DE','DC','FL','GA','MD','NC','SC','VA','WV',\n",
    "            'AL','KY','MS','TN','AR','LA','OK','TX'\n",
    "        ], 'south'),\n",
    "        # West\n",
    "        **dict.fromkeys([\n",
    "            'AZ','CO','ID','MT','NV','NM','UT','WY',\n",
    "            'AK','CA','HI','OR','WA'\n",
    "        ], 'west')\n",
    "    }\n",
    "    curr_region = state_to_region[curr_state]\n",
    "    \n",
    "    mask = (combined_cpi_df['Year'] == curr_year) & (combined_cpi_df['Month'] == curr_month) & (combined_cpi_df['Region'] == curr_region)\n",
    "    ml_df.loc[idx, 'INFLATIONRATE'] = combined_cpi_df.loc[mask, 'INFLATIONRATE'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf84c328-bce4-401a-a206-7f81583d6cd2",
   "metadata": {},
   "source": [
    "Economic Policy Uncertainty (EPU) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "id": "982f8212-56c3-4571-9c84-ae44df18e947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import EPU dataset\n",
    "file_path = 'macroeconomy_dataset/epu_dataset/epu_dataset.xlsx'\n",
    "epu_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "id": "d6ed325b-80c2-4234-8745-6c9002d5ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep the EPU_Composite\n",
    "for col in epu_df.columns:\n",
    "    if re.findall(f'EPU_Composite..', col) or col == 'year' or col == 'month':\n",
    "        pass\n",
    "    else:\n",
    "        epu_df.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "# Rename the columns\n",
    "new_cols = ['year', 'month']\n",
    "epu_df.columns = new_cols + [col[-2:] for col in epu_df.columns[2:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "95b055af-5656-434e-8b8b-85753a5a845c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the EPU into ml_df\n",
    "ml_df['EPU'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_month = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).month\n",
    "    mask = (epu_df['year'] == curr_year) & (epu_df['month'] == curr_month)\n",
    "    ml_df.loc[idx, 'EPU'] = epu_df.loc[mask, curr_state].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc265399-bc65-43bd-b2a3-0eb4c1e6553d",
   "metadata": {},
   "source": [
    "Consumer Sentiment Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "id": "a705d19d-28a0-4187-bd26-2dfb089baf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Consumner Sentiment index dataset\n",
    "file_path = 'macroeconomy_dataset/sentiment_dataset/sentiment_dataset.xlsx'\n",
    "sentiment_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "d3a77764-618b-4e78-84d3-39f238f48c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Consumer Sentiment index into ml_df\n",
    "ml_df['CONSUMERSENTIMENT'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_month = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).month\n",
    "    mask = (sentiment_df['Year'] == curr_year) & (sentiment_df['Month'] == curr_month)\n",
    "    ml_df.loc[idx, 'CONSUMERSENTIMENT'] = sentiment_df.loc[mask, 'Index'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a893f2c-ef54-4eeb-bd8c-c9980fc4d634",
   "metadata": {},
   "source": [
    "Expected Change in Unemployment Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "dc3e361c-d988-418a-bc3a-d12cdbdd1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Expected Change in Unemployment dataset\n",
    "file_path = 'macroeconomy_dataset/expected_change_unemployment/expected_change_unemployment.xlsx'\n",
    "exp_unemployment_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "ad925394-7d3f-44d6-9723-809d59ca3298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the CExpected Change in Unemployment dataset into ml_df\n",
    "ml_df['EX_UNEMPLOYMENTRATE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_month = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).month\n",
    "    mask = (exp_unemployment_df['Year'] == curr_year) & (exp_unemployment_df['Month'] == curr_month)\n",
    "    ml_df.loc[idx, 'EX_UNEMPLOYMENTRATE'] = exp_unemployment_df.loc[mask, 'Relative'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a35677-cc07-4103-8aac-33972429bbef",
   "metadata": {},
   "source": [
    "Expected Change in Interest Rates Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "af95610a-4e66-467b-a77c-e2ee4f7a927d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Expected Change in Interest Rate dataset\n",
    "file_path = 'macroeconomy_dataset/expected_change_interest/expected_change_interest.xlsx'\n",
    "exp_interest_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "83591b49-0a9e-43d5-a716-d1b309c7acf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Expected Change in Interest Rate into ml_df\n",
    "ml_df['EX_SPOTINTERESTRATE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_month = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).month\n",
    "    mask = (exp_interest_df['Year'] == curr_year) & (exp_interest_df['Month'] == curr_month)\n",
    "    ml_df.loc[idx, 'EX_SPOTINTERESTRATE'] = exp_interest_df.loc[mask, 'Relative'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc84f5c-c61f-44cf-965a-0ca054726292",
   "metadata": {},
   "source": [
    "Expected Change in Prices (Inflation) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "07e8874d-0965-4b4c-8835-44140cc78fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Expected Change in Price dataset\n",
    "file_path = 'macroeconomy_dataset/expected_change_price/expected_change_price.xlsx'\n",
    "exp_inflation_df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "30c9ab2e-331d-4e55-8743-889ddc6c9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Expected Change in Price into ml_df\n",
    "ml_df['EX_INFLATIONRATE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_month = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).month\n",
    "    mask = (exp_inflation_df['Year'] == curr_year) & (exp_inflation_df['Month'] == curr_month)\n",
    "    ml_df.loc[idx, 'EX_INFLATIONRATE'] = exp_inflation_df.loc[mask, 'Mean'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d6c2e-ec0d-48a3-a70e-4a49c3daa2ab",
   "metadata": {},
   "source": [
    "Gini Index Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "39ba36c6-83b3-4442-ae56-b404f7071088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Gini Index dataset\n",
    "file_path = 'macroeconomy_dataset/gini_dataset/gini_dataset.xlsx'\n",
    "gini_df = pd.read_excel(file_path)\n",
    "\n",
    "# Add state code column\n",
    "state_name_to_code = {\n",
    "    'united states':        'US',\n",
    "    'alabama':              'AL',\n",
    "    'alaska':               'AK',\n",
    "    'arizona':              'AZ',\n",
    "    'arkansas':             'AR',\n",
    "    'california':           'CA',\n",
    "    'colorado':             'CO',\n",
    "    'connecticut':          'CT',\n",
    "    'delaware':             'DE',\n",
    "    'district of columbia': 'DC',\n",
    "    'florida':              'FL',\n",
    "    'georgia':              'GA',\n",
    "    'hawaii':               'HI',\n",
    "    'idaho':                'ID',\n",
    "    'illinois':             'IL',\n",
    "    'indiana':              'IN',\n",
    "    'iowa':                 'IA',\n",
    "    'kansas':               'KS',\n",
    "    'kentucky':             'KY',\n",
    "    'louisiana':            'LA',\n",
    "    'maine':                'ME',\n",
    "    'maryland':             'MD',\n",
    "    'massachusetts':        'MA',\n",
    "    'michigan':             'MI',\n",
    "    'minnesota':            'MN',\n",
    "    'mississippi':          'MS',\n",
    "    'missouri':             'MO',\n",
    "    'montana':              'MT',\n",
    "    'nebraska':             'NE',\n",
    "    'nevada':               'NV',\n",
    "    'new hampshire':        'NH',\n",
    "    'new jersey':           'NJ',\n",
    "    'new mexico':           'NM',\n",
    "    'new york':             'NY',\n",
    "    'north carolina':       'NC',\n",
    "    'north dakota':         'ND',\n",
    "    'ohio':                 'OH',\n",
    "    'oklahoma':             'OK',\n",
    "    'oregon':               'OR',\n",
    "    'pennsylvania':         'PA',\n",
    "    'rhode island':         'RI',\n",
    "    'south carolina':       'SC',\n",
    "    'south dakota':         'SD',\n",
    "    'tennessee':            'TN',\n",
    "    'texas':                'TX',\n",
    "    'utah':                 'UT',\n",
    "    'vermont':              'VT',\n",
    "    'virginia':             'VA',\n",
    "    'washington':           'WA',\n",
    "    'west virginia':        'WV',\n",
    "    'wisconsin':            'WI',\n",
    "    'wyoming':              'WY',\n",
    "    'puerto rico':          'PR'\n",
    "}\n",
    "gini_df['State'] = gini_df['Location'].str.lower().map(state_name_to_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "707abc8b-ab53-4d65-84e1-f0bea997f243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is an absent on data point for the year 2020\n",
    "# 1) Build a full MultiIndex of every Location × every Year you care about\n",
    "locations = gini_df['Location'].unique()\n",
    "years     = np.arange(2016, 2024)            # adjust to your overall span\n",
    "full_index = pd.MultiIndex.from_product(\n",
    "    [locations, years],\n",
    "    names=['Location','TimeFrame']\n",
    ")\n",
    "\n",
    "# 2) Reindex your DataFrame onto that full grid (this will insert a 2020 row per Location, with Gini = NaN)\n",
    "gini_df = (\n",
    "    gini_df\n",
    "      .set_index(['Location','TimeFrame'])         # pivot into the MultiIndex\n",
    "      .reindex(full_index)                    # insert missing years\n",
    ")\n",
    "\n",
    "# 3) Now interpolate within each Location\n",
    "gini_df['Gini_imputed'] = (\n",
    "    gini_df\n",
    "      .groupby(level='Location')['Data']\n",
    "      .apply(lambda s: s.interpolate(method='linear'))\n",
    ")\n",
    "\n",
    "# 4) Perform forward fill on the rest of the columns\n",
    "gini_df.ffill(inplace=True)\n",
    "\n",
    "# bring Year & Location back as columns\n",
    "gini_df = gini_df.reset_index()\n",
    "\n",
    "# Dtop the 'Data' column\n",
    "gini_df.drop('Data', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "86c316fe-ac91-440d-b57f-70c1790e5529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the Gini Index into ml_df\n",
    "ml_df['GINI'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    mask = (gini_df['TimeFrame'] == curr_year) & (gini_df['State'] == curr_state)\n",
    "    ml_df.loc[idx, 'GINI'] = gini_df.loc[mask, 'Gini_imputed'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86379f90-467d-4f58-92cf-3373cdc2b6d1",
   "metadata": {},
   "source": [
    "Geopolitical Risk (GPR) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "40ab3c7d-0b55-49f0-acb3-4f995fdbe984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import GPR dataset\n",
    "file_path = 'macroeconomy_dataset/geopolitical_risk_dataset/data_gpr_export.xls'\n",
    "gpr_df = pd.read_excel(file_path)\n",
    "\n",
    "# Only include relevant columns\n",
    "gpr_df = gpr_df.loc[:, ['month', 'GPRHC_USA']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "4ffcde31-c399-49b5-a07e-89713be9ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the GPR into ml_df\n",
    "ml_df['GPR'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_year = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).year\n",
    "    curr_month = (ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)).month\n",
    "    mask = (gpr_df['month'].dt.year == curr_year) & (gpr_df['month'].dt.month == curr_month)\n",
    "    ml_df.loc[idx, 'GPR'] = gpr_df.loc[mask, 'GPRHC_USA'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516b3b6-b0f6-458b-b608-b5651041fb89",
   "metadata": {},
   "source": [
    "VIX Index and S&P500 Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "e6ec7dd4-ec82-4ee5-a8fa-33d7997c2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "365c9e1e-9937-4772-9585-ce8405e4594e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import s&p and vix dataset\n",
    "vix_snp_df = pd.read_csv('macroeconomy_dataset/s&p_vix_dataset/s&p_vix_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "bcbc2dc3-5c26-476f-9782-ba5bb89b9ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute returns of snp\n",
    "vix_snp_df['S&P500_RETURN'] = vix_snp_df['S&P500'].pct_change()\n",
    "\n",
    "# Compute the 30-day average of S&P500 return\n",
    "vix_snp_df['S&P500_RETURN_AVG'] = vix_snp_df['S&P500_RETURN'].rolling(window=30, min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "fdeae7fd-1a3e-4636-a482-9e1032acee55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use forward-fill to fill up average interest rate during the weekend / non-trading days\n",
    "vix_snp_df.index = pd.to_datetime(vix_snp_df['Date'])\n",
    "full_date = pd.date_range(start=vix_snp_df.index.min(),\n",
    "                         end=vix_snp_df.index.max(),\n",
    "                         freq='D')\n",
    "vix_snp_df = vix_snp_df.reindex(full_date).ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "e5191919-26f2-480b-8d83-4279caca292a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute the VIX into ml_df\n",
    "ml_df['VIX'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_date = ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)\n",
    "    ml_df.loc[idx, 'VIX'] = vix_snp_df.loc[curr_date, 'VIX']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "eadd3919-9dee-4e69-b99e-9ea137c48122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the SNP into ml_df\n",
    "ml_df['SNP'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_date = ml_df.loc[idx, 'FILING_DATE'] + pd.Timedelta(days=21)\n",
    "    ml_df.loc[idx, 'SNP'] = vix_snp_df.loc[curr_date, 'S&P500_RETURN_AVG']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147f2944-08f1-4652-a679-465302d43d87",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Additional Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b527b9b-aa38-48cb-9cba-e0ffa3f4f202",
   "metadata": {},
   "source": [
    "Add Population Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "340780be-3a48-48f4-9fa8-cb6f748be0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import population\n",
    "file_path = 'demographic_dataset/population_dataset/population_dataset.xlsx'\n",
    "population_df = pd.read_excel(file_path)\n",
    "\n",
    "# Add the population to ml_df\n",
    "ml_df['POPULATION'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = ml_df.loc[idx, 'FILING_DATE'].year\n",
    "    mask = (population_df['Year'] == curr_year)\n",
    "    ml_df.loc[idx, 'POPULATION'] = population_df.loc[mask, curr_state].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04482cc-809a-4202-bd10-e5b2e6997fce",
   "metadata": {},
   "source": [
    "Add House Pricing Index (HPI) Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "54517134-f684-4b03-ad3e-a31447a5b11c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import HPI index\n",
    "file_path = 'macroeconomy_dataset/hpi_dataset/hpi_dataset.xlsx'\n",
    "hpi_df = pd.read_excel(file_path)\n",
    "\n",
    "# Add the HPI index to ml_df\n",
    "ml_df['HPI'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    curr_state = ml_df.loc[idx, 'STATEORCOUNTRY']\n",
    "    curr_year = ml_df.loc[idx, 'FILING_DATE'].year\n",
    "    curr_quarter = ml_df.loc[idx, 'FILING_DATE'].quarter\n",
    "    mask = (hpi_df['yr'] == curr_year) & (hpi_df['period'] == curr_quarter) & (hpi_df['place_id'] == curr_state)\n",
    "    ml_df.loc[idx, 'HPI'] = hpi_df.loc[mask, 'index_sa'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb4e344-414c-4d6d-969a-061fabd90db5",
   "metadata": {},
   "source": [
    "Add Covid Indicator Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "671ed2f4-112d-4bce-939e-41bf36998fb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Covid period\n",
    "covid_start = pd.Timestamp('2020-03-01')\n",
    "covid_end = pd.Timestamp('2021-06-30')\n",
    "\n",
    "# Add covid indicator feature\n",
    "ml_df['IS_COVID'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    if (ml_df.loc[idx, 'FILING_DATE'] >= covid_start) & (ml_df.loc[idx, 'FILING_DATE'] <= covid_end):\n",
    "        ml_df.loc[idx, 'IS_COVID'] = 1\n",
    "    else:\n",
    "        ml_df.loc[idx, 'IS_COVID'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a050e-b049-4c8f-836a-123eea0f2649",
   "metadata": {},
   "source": [
    "Add Post-SEC Regulation Change Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "a6e8c450-2add-4557-bef9-c33b3480f409",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the date when Reg CF regulation was amended\n",
    "change_date = pd.Timestamp('2021-03-26')\n",
    "\n",
    "# Add post-change feature\n",
    "ml_df['IS_POSTREGCHANGE'] = np.nan\n",
    "for idx in ml_df.index:\n",
    "    if ml_df.loc[idx, 'FILING_DATE'] >= change_date:\n",
    "        ml_df.loc[idx, 'IS_POSTREGCHANGE'] = 1\n",
    "    else:\n",
    "        ml_df.loc[idx, 'IS_POSTREGCHANGE'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c278c9-bbfa-490b-8d65-e5f72117ceb1",
   "metadata": {},
   "source": [
    "Add Cash Runway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "fe3e519b-28a4-4fb0-8a9c-81a2af4fe9c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute monthly net burn rate\n",
    "ml_df['NETBURNRATE'] = (ml_df['NETINCOME'] * -1)/12\n",
    "\n",
    "# Compute cash runway (in month)\n",
    "ml_df['CASHRUNWAY'] = np.arcsinh((ml_df['CASH']) / (ml_df['NETBURNRATE'] + 1e-10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e3e15-de32-4fc2-b844-287b504feb1a",
   "metadata": {},
   "source": [
    "### Cleaning The ml_df Dataset (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e52a606-c4b0-4955-b8d8-5a2feba348f3",
   "metadata": {},
   "source": [
    "Only include CF offerings thst end between 2018 Q1 and 2023 Q4 (This had been completed previously).\n",
    "\n",
    "Only include CF offerings that start between 2018 Q1 and 2023 Q4 (This is just completed below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "44ba9781-4334-41f1-8b45-19393b239d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_2 = ml_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "b3b40ad9-f5e1-4bca-865e-29e83b26726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure to only include CF offerings that start between 2018 Q1 and 2023 Q4\n",
    "for file_number in ml_df_2.index:\n",
    "    if ml_df_2.loc[file_number, 'FILING_DATE'] >= pd.Timestamp('2018-01-01'):\n",
    "        pass\n",
    "    else:\n",
    "        ml_df_2.drop(index=file_number, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1663e9b-bd78-4acf-a9c8-340a380d5059",
   "metadata": {},
   "source": [
    "Only include relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "3613f112-a4de-477e-b237-f12c25c59f46",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Y', 'REVENUE', 'PREVIOUSREVENUE', 'ASSET', 'CASH', 'LONGTERMDEBT',\n",
       "       'NETINCOME', 'SHORTTERMDEBT', 'ACCREC', 'COGS', 'MINIMUMSIZE',\n",
       "       'TEXT_RAISEDCOMMISSION', 'TEXT_EQUITYCOMMISSION', 'COMMISSIONCIK',\n",
       "       'FILING_DATE', 'FINALDEADLINEDATE', 'FINALOFFERINGAMOUNT',\n",
       "       'SECURITYOFFEREDTYPE', 'SECURITYOFFEREDOTHERDESC',\n",
       "       'PRICEDETERMINATIONMETHOD', 'DATEINCORPORATION', 'CIK',\n",
       "       'ISSUERSIGNATURE', 'CURRENTEMPLOYEES', 'LEGALSTATUSFORM',\n",
       "       'LEGALSTATUSOTHERDESC', 'JURISDICTIONORGANIZATION', 'STATEORCOUNTRY',\n",
       "       'REVENUEGROWTH', 'DEBTTOASSET', 'WORKINGCAPITAL', 'ROA', 'GROSSPROFIT',\n",
       "       'GROSSMARGIN', 'NETPROFITMARGIN', 'RAISEDCOMMISSION',\n",
       "       'EQUITYCOMMISSION', 'PLATFORMPOPULARITY', 'DURATION',\n",
       "       'COMPETITORSCOUNT', 'EXTRACTED_SECURITYOFFEREDOTHERDESC',\n",
       "       'FINALSECURITYOFFEREDTYPE', 'IS_PRICEMETHOD', 'COMPANYAGE',\n",
       "       'PASTCOMPANYSUCCESS', 'PASTCOMPANYFAILURE', 'PASTPERSONSUCCESS',\n",
       "       'PASTPERSONFAILURE', 'ISDELAWARE', 'LOCALCOMPETITORSCOUNT',\n",
       "       'CREATIVEWORKFORCE', 'RUCC', 'INCOMELEVEL', 'AGE', 'BACHELOREDUCATION',\n",
       "       'UNEMPLOYMENTRATE', 'SPOTINTERESTRATE', 'AVERAGEINTERESTRATE',\n",
       "       'INFLATIONRATE', 'EPU', 'CONSUMERSENTIMENT', 'EX_UNEMPLOYMENTRATE',\n",
       "       'EX_SPOTINTERESTRATE', 'EX_INFLATIONRATE', 'GINI', 'GPR', 'VIX', 'SNP',\n",
       "       'POPULATION', 'HPI', 'IS_COVID', 'IS_POSTREGCHANGE', 'NETBURNRATE',\n",
       "       'CASHRUNWAY'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "d3938483-643c-4e43-b909-e034b5459e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all relevant columns\n",
    "relevant_columns = [\n",
    "    'Y',\n",
    "    'WORKINGCAPITAL','REVENUE','REVENUEGROWTH','DEBTTOASSET','ROA','ASSET','CASH','LONGTERMDEBT','NETINCOME','GROSSPROFIT','GROSSMARGIN','NETPROFITMARGIN','CASHRUNWAY',\n",
    "    'UNEMPLOYMENTRATE','AVERAGEINTERESTRATE','INFLATIONRATE','EX_UNEMPLOYMENTRATE','EX_SPOTINTERESTRATE','EX_INFLATIONRATE','EPU','CONSUMERSENTIMENT','VIX','SNP','GINI','GPR','HPI',\n",
    "    'CREATIVEWORKFORCE','RUCC','AGE','INCOMELEVEL','BACHELOREDUCATION', 'POPULATION',\n",
    "    'MINIMUMSIZE','RAISEDCOMMISSION','EQUITYCOMMISSION','PLATFORMPOPULARITY','DURATION','COMPETITORSCOUNT', 'LOCALCOMPETITORSCOUNT', 'FINALOFFERINGAMOUNT','FINALSECURITYOFFEREDTYPE','IS_PRICEMETHOD','IS_COVID', 'IS_POSTREGCHANGE',\n",
    "    'COMPANYAGE','PASTCOMPANYSUCCESS','PASTCOMPANYFAILURE','PASTPERSONSUCCESS','PASTPERSONFAILURE','CURRENTEMPLOYEES','ISDELAWARE'\n",
    "]\n",
    "\n",
    "ml_df_2 = ml_df_2.loc[:, relevant_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d366149-853e-4e84-942f-fd50d30b4f68",
   "metadata": {},
   "source": [
    "Check for nulls in the features and then drop those null rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "b75665b6-1a70-45bd-abf6-4f1647891f8a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Y                             0\n",
       "WORKINGCAPITAL                0\n",
       "REVENUE                       0\n",
       "REVENUEGROWTH                 0\n",
       "DEBTTOASSET                   0\n",
       "ROA                           0\n",
       "ASSET                         0\n",
       "CASH                          0\n",
       "LONGTERMDEBT                  0\n",
       "NETINCOME                     0\n",
       "GROSSPROFIT                   0\n",
       "GROSSMARGIN                   0\n",
       "NETPROFITMARGIN               0\n",
       "CASHRUNWAY                    0\n",
       "UNEMPLOYMENTRATE              0\n",
       "AVERAGEINTERESTRATE           0\n",
       "INFLATIONRATE                 0\n",
       "EX_UNEMPLOYMENTRATE           0\n",
       "EX_SPOTINTERESTRATE           0\n",
       "EX_INFLATIONRATE              0\n",
       "EPU                           0\n",
       "CONSUMERSENTIMENT             0\n",
       "VIX                           0\n",
       "SNP                           0\n",
       "GINI                          0\n",
       "GPR                           0\n",
       "HPI                           0\n",
       "CREATIVEWORKFORCE             0\n",
       "RUCC                          0\n",
       "AGE                           0\n",
       "INCOMELEVEL                   0\n",
       "BACHELOREDUCATION             0\n",
       "POPULATION                    0\n",
       "MINIMUMSIZE                 195\n",
       "RAISEDCOMMISSION              0\n",
       "EQUITYCOMMISSION              0\n",
       "PLATFORMPOPULARITY            0\n",
       "DURATION                      0\n",
       "COMPETITORSCOUNT              0\n",
       "LOCALCOMPETITORSCOUNT         0\n",
       "FINALOFFERINGAMOUNT           0\n",
       "FINALSECURITYOFFEREDTYPE      0\n",
       "IS_PRICEMETHOD                0\n",
       "IS_COVID                      0\n",
       "IS_POSTREGCHANGE              0\n",
       "COMPANYAGE                    0\n",
       "PASTCOMPANYSUCCESS            0\n",
       "PASTCOMPANYFAILURE            0\n",
       "PASTPERSONSUCCESS             0\n",
       "PASTPERSONFAILURE             0\n",
       "CURRENTEMPLOYEES              0\n",
       "ISDELAWARE                    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for nulls\n",
    "ml_df_2.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "201a64f6-0c00-4a86-90ab-eb386a1cab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data points with missing 'MINIMUMSIZE' and 'REVENUEGROWTH' = np.nan (due to negative disclosed REVENUE) are removed\n",
    "ml_df_2.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d023d799-6e87-4395-a750-9c76020329d9",
   "metadata": {},
   "source": [
    "Export to CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "f420e4cf-8cef-44e6-b6cb-60a94ba0691b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_df_2.to_csv('ml_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "50d43126-40f4-4e5a-9c94-ea4415c09257",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>WORKINGCAPITAL</th>\n",
       "      <th>REVENUE</th>\n",
       "      <th>REVENUEGROWTH</th>\n",
       "      <th>DEBTTOASSET</th>\n",
       "      <th>ROA</th>\n",
       "      <th>ASSET</th>\n",
       "      <th>CASH</th>\n",
       "      <th>LONGTERMDEBT</th>\n",
       "      <th>NETINCOME</th>\n",
       "      <th>...</th>\n",
       "      <th>IS_PRICEMETHOD</th>\n",
       "      <th>IS_COVID</th>\n",
       "      <th>IS_POSTREGCHANGE</th>\n",
       "      <th>COMPANYAGE</th>\n",
       "      <th>PASTCOMPANYSUCCESS</th>\n",
       "      <th>PASTCOMPANYFAILURE</th>\n",
       "      <th>PASTPERSONSUCCESS</th>\n",
       "      <th>PASTPERSONFAILURE</th>\n",
       "      <th>CURRENTEMPLOYEES</th>\n",
       "      <th>ISDELAWARE</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FILE_NUMBER</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>020-23892</th>\n",
       "      <td>1.0</td>\n",
       "      <td>5960.00</td>\n",
       "      <td>90802.0</td>\n",
       "      <td>11.416448</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.082502</td>\n",
       "      <td>258379.00</td>\n",
       "      <td>2460.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-21341.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-23894</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-13075.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-8.987572</td>\n",
       "      <td>6.081844</td>\n",
       "      <td>-6.672245</td>\n",
       "      <td>60.00</td>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-23705.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4464</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-23896</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-195005.00</td>\n",
       "      <td>30747.0</td>\n",
       "      <td>10.333580</td>\n",
       "      <td>0.628790</td>\n",
       "      <td>-1.060695</td>\n",
       "      <td>329742.00</td>\n",
       "      <td>2961.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-419129.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3373</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-23900</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1195.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.904443</td>\n",
       "      <td>1195.00</td>\n",
       "      <td>1195.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-80591.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2934</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-23904</th>\n",
       "      <td>1.0</td>\n",
       "      <td>35977.16</td>\n",
       "      <td>21487.2</td>\n",
       "      <td>1.457424</td>\n",
       "      <td>0.100068</td>\n",
       "      <td>-0.513261</td>\n",
       "      <td>38692.43</td>\n",
       "      <td>35977.16</td>\n",
       "      <td>3878.33</td>\n",
       "      <td>-20742.82</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-33264</th>\n",
       "      <td>0.0</td>\n",
       "      <td>54225.00</td>\n",
       "      <td>607816.0</td>\n",
       "      <td>2.208979</td>\n",
       "      <td>0.632368</td>\n",
       "      <td>0.016843</td>\n",
       "      <td>94577.00</td>\n",
       "      <td>63463.00</td>\n",
       "      <td>54636.00</td>\n",
       "      <td>1593.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>903</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-33266</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-120510.00</td>\n",
       "      <td>92456.0</td>\n",
       "      <td>11.434499</td>\n",
       "      <td>2.753577</td>\n",
       "      <td>-4.597427</td>\n",
       "      <td>17276.00</td>\n",
       "      <td>14545.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-857050.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-33272</th>\n",
       "      <td>1.0</td>\n",
       "      <td>89047.00</td>\n",
       "      <td>1880819.0</td>\n",
       "      <td>-0.240243</td>\n",
       "      <td>0.606216</td>\n",
       "      <td>0.194890</td>\n",
       "      <td>1808372.00</td>\n",
       "      <td>156207.00</td>\n",
       "      <td>1097494.00</td>\n",
       "      <td>354669.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7510</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-33275</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2284.00</td>\n",
       "      <td>285282.0</td>\n",
       "      <td>0.129512</td>\n",
       "      <td>2.092650</td>\n",
       "      <td>-0.907112</td>\n",
       "      <td>92147.00</td>\n",
       "      <td>7182.00</td>\n",
       "      <td>362907.00</td>\n",
       "      <td>-95532.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2224</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>020-33279</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-17559.00</td>\n",
       "      <td>131518.0</td>\n",
       "      <td>11.786907</td>\n",
       "      <td>3.498948</td>\n",
       "      <td>-3.744357</td>\n",
       "      <td>1131.00</td>\n",
       "      <td>1131.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-23897.00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1481</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3563 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Y  WORKINGCAPITAL    REVENUE  REVENUEGROWTH  DEBTTOASSET  \\\n",
       "FILE_NUMBER                                                               \n",
       "020-23892    1.0         5960.00    90802.0      11.416448     0.000000   \n",
       "020-23894    1.0       -13075.00        0.0      -8.987572     6.081844   \n",
       "020-23896    1.0      -195005.00    30747.0      10.333580     0.628790   \n",
       "020-23900    1.0         1195.00        0.0       0.000000     0.000000   \n",
       "020-23904    1.0        35977.16    21487.2       1.457424     0.100068   \n",
       "...          ...             ...        ...            ...          ...   \n",
       "020-33264    0.0        54225.00   607816.0       2.208979     0.632368   \n",
       "020-33266    0.0      -120510.00    92456.0      11.434499     2.753577   \n",
       "020-33272    1.0        89047.00  1880819.0      -0.240243     0.606216   \n",
       "020-33275    0.0         2284.00   285282.0       0.129512     2.092650   \n",
       "020-33279    0.0       -17559.00   131518.0      11.786907     3.498948   \n",
       "\n",
       "                  ROA       ASSET       CASH  LONGTERMDEBT  NETINCOME  ...  \\\n",
       "FILE_NUMBER                                                            ...   \n",
       "020-23892   -0.082502   258379.00    2460.00          0.00  -21341.00  ...   \n",
       "020-23894   -6.672245       60.00      60.00          0.00  -23705.00  ...   \n",
       "020-23896   -1.060695   329742.00    2961.00          0.00 -419129.00  ...   \n",
       "020-23900   -4.904443     1195.00    1195.00          0.00  -80591.00  ...   \n",
       "020-23904   -0.513261    38692.43   35977.16       3878.33  -20742.82  ...   \n",
       "...               ...         ...        ...           ...        ...  ...   \n",
       "020-33264    0.016843    94577.00   63463.00      54636.00    1593.00  ...   \n",
       "020-33266   -4.597427    17276.00   14545.00          0.00 -857050.00  ...   \n",
       "020-33272    0.194890  1808372.00  156207.00    1097494.00  354669.00  ...   \n",
       "020-33275   -0.907112    92147.00    7182.00     362907.00  -95532.00  ...   \n",
       "020-33279   -3.744357     1131.00    1131.00          0.00  -23897.00  ...   \n",
       "\n",
       "             IS_PRICEMETHOD  IS_COVID  IS_POSTREGCHANGE  COMPANYAGE  \\\n",
       "FILE_NUMBER                                                           \n",
       "020-23892               0.0       0.0               0.0        1112   \n",
       "020-23894               1.0       0.0               0.0        4464   \n",
       "020-23896               0.0       0.0               0.0        3373   \n",
       "020-23900               0.0       0.0               0.0        2934   \n",
       "020-23904               0.0       0.0               0.0        1756   \n",
       "...                     ...       ...               ...         ...   \n",
       "020-33264               1.0       0.0               1.0         903   \n",
       "020-33266               1.0       0.0               1.0        1146   \n",
       "020-33272               1.0       0.0               1.0        7510   \n",
       "020-33275               1.0       0.0               1.0        2224   \n",
       "020-33279               1.0       0.0               1.0        1481   \n",
       "\n",
       "             PASTCOMPANYSUCCESS  PASTCOMPANYFAILURE  PASTPERSONSUCCESS  \\\n",
       "FILE_NUMBER                                                              \n",
       "020-23892                   0.0                 0.0                0.0   \n",
       "020-23894                   0.0                 0.0                0.0   \n",
       "020-23896                   1.0                 0.0                1.0   \n",
       "020-23900                   1.0                 0.0                1.0   \n",
       "020-23904                   0.0                 0.0                0.0   \n",
       "...                         ...                 ...                ...   \n",
       "020-33264                   0.0                 0.0                0.0   \n",
       "020-33266                   0.0                 0.0                0.0   \n",
       "020-33272                   1.0                 0.0                1.0   \n",
       "020-33275                   0.0                 0.0                0.0   \n",
       "020-33279                   0.0                 0.0                0.0   \n",
       "\n",
       "             PASTPERSONFAILURE  CURRENTEMPLOYEES  ISDELAWARE  \n",
       "FILE_NUMBER                                                   \n",
       "020-23892                  0.0               1.0         0.0  \n",
       "020-23894                  0.0              11.0         1.0  \n",
       "020-23896                  0.0               1.0         0.0  \n",
       "020-23900                  0.0               4.0         0.0  \n",
       "020-23904                  1.0               5.0         0.0  \n",
       "...                        ...               ...         ...  \n",
       "020-33264                  0.0              10.0         0.0  \n",
       "020-33266                  0.0               1.0         0.0  \n",
       "020-33272                  0.0              25.0         0.0  \n",
       "020-33275                  0.0              10.0         0.0  \n",
       "020-33279                  0.0               6.0         0.0  \n",
       "\n",
       "[3563 rows x 52 columns]"
      ]
     },
     "execution_count": 465,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc8fc7-ced1-438e-84dc-33433d8e7a02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
